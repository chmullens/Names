{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminology point:\n",
    "\n",
    "This project relies quite a bit on how names are categorized by the Social Security Administration. Each birth is registered as either male or female. Throughout the text, these terms refer to assigned sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's been talked about so often times it's hard to even find a \n",
    "#source, but the idea of \"unique but familiar\" is all over in\n",
    "#marketing. Probably the most famous phrasing is \"Most Advanced \n",
    "#Yet Acceptable\", coined by Raymond Loewy. Basically, people \n",
    "#like things that are new, but fit somewhere they understand.\n",
    "\n",
    "#For this project, my goal is to let users find names that fit.\n",
    "#If a name tells you right off the bat that you're looking at a\n",
    "#specific time period or age range, that's what I'm going for.\n",
    "\n",
    "#Say you want to market something to people around 65. Do you\n",
    "#want to pick \"Jacob\" as a name in an ad? Probably not. The \n",
    "#vast majority of people named \"Jacob\" are under 40. \n",
    "\n",
    "#\"Most Advanced Yet Acceptable\"\n",
    "#https://www.theatlantic.com/magazine/archive/2017/01/what-makes-things-cool/508772/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "#I've been having trouble with the autocomplete, which is\n",
    "#flat-out failing, but turning off the newer(?) version fixes\n",
    "#it completely.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import colorsys\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import datetime as dt\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "pd.set_option('display.max_rows',150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set figure options:\n",
    "\n",
    "#Yes, I want this as a universal default across all notebooks.\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Helvetica']\n",
    "\n",
    "plt.rc('font', size=12)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=16)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=12)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=12)    # fontsize of the tick labels\n",
    "# plt.rc('legend', fontsize=12)    # legend fontsize\n",
    "# plt.rc('figure', titlesize=10)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'names_df_trim.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-315a75449d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtotalnames_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total_soc_cards.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnames_df_trim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'names_df_trim.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnamelife_F_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'namelife_F_full.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     ) as handles:\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'names_df_trim.pkl'"
     ]
    }
   ],
   "source": [
    "#Load in preprocessed data. The \"namelife_S_full\" files are by\n",
    "#far the largest, at 3-4 gigs.\n",
    "\n",
    "alive_F = pd.read_pickle('/Users/chmullens/Documents/python_work/Actuarial_tables/life_F_df.pkl')\n",
    "alive_M = pd.read_pickle('/Users/chmullens/Documents/python_work/Actuarial_tables/life_M_df.pkl')\n",
    "alive_F_p = pd.read_pickle('/Users/chmullens/Documents/python_work/Actuarial_tables/life_F_p_df.pkl')\n",
    "alive_M_p = pd.read_pickle('/Users/chmullens/Documents/python_work/Actuarial_tables/life_M_p_df.pkl')\n",
    "\n",
    "#alive_M_t = alive_M.values.astype('float64').T\n",
    "#alive_F_t = alive_F.values.astype('float64').T\n",
    "\n",
    "totalnames_table = pd.read_pickle('Total_soc_cards.pkl')\n",
    "names_df_trim = pd.read_pickle('names_df_trim.pkl')\n",
    "\n",
    "namelife_F_full = np.load('namelife_F_full.npy')\n",
    "namelife_F_base = np.load('namelife_F_base.npy')\n",
    "namelife_F_name = pd.read_pickle('namelife_F_name.pkl')\n",
    "namebirth_F = np.load('namebirth_F.npy')\n",
    "namelife_M_full = np.load('namelife_M_full.npy')\n",
    "namelife_M_base = np.load('namelife_M_base.npy')\n",
    "namelife_M_name = pd.read_pickle('namelife_M_name.pkl')\n",
    "namebirth_M = np.load('namebirth_M.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL VISUALIZATION.\n",
    "\n",
    "#Set variable here to \n",
    "showplot = False\n",
    "\n",
    "if showplot:\n",
    "    #Example name:\n",
    "    tgt_name = 'Lakynn'\n",
    "    tgt_sex = 'F'\n",
    "\n",
    "    if tgt_sex == 'M':\n",
    "        tempy = namelife_M_base[np.where(namelife_M_name==tgt_name),:][0,0,:]\n",
    "    else:\n",
    "        tempy = namelife_F_base[np.where(namelife_F_name==tgt_name),:][0,0,:]\n",
    "    plt.plot(np.arange(1880, 2050), tempy)\n",
    "    tempdf = names_df_trim[(names_df_trim['Name']==tgt_name) & (names_df_trim['Sex']==tgt_sex)]\n",
    "    plt.plot(tempdf['Year'], tempdf['Number'])\n",
    "    plt.plot([2019, 2019],[0,1.1*np.max(tempy)],color=[0,0,0,0.1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL VISUALIZATION: THIS ONE IS HANDY\n",
    "\n",
    "if 1:\n",
    "    #Example life of name:\n",
    "\n",
    "    nameind = 1948\n",
    "\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    im = plt.imshow(namelife_F_full[nameind,:,:])\n",
    "    ax.set_title('\\n (Living \"' + str(namelife_F_name[nameind]) + '\"s in X year, with Y birth year)')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_xticks(np.arange(0,2050-1880,10))\n",
    "    ax.set_xticklabels(np.arange(1880,2050,10), rotation=90)\n",
    "\n",
    "    plt.ylabel('Birth year (1880-2019)')\n",
    "    plt.colorbar(im)\n",
    "    plt.show()\n",
    "\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    ax.plot(namebirth_F[nameind,:])\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(namelife_F_base[nameind,:], color='red')\n",
    "    # ax.plot([139, 139],[0,4.5e10],color=[0,0,0,0.1])\n",
    "    #ax.set_xlim([0,138])\n",
    "    year_x = [0,20,40,60,80,100,120,140,160,180]\n",
    "    ax.set_xticks(year_x)\n",
    "    year_tick = (np.array(year_x)+1880).astype('str')\n",
    "    ax.set_xticklabels(year_tick)\n",
    "    ax.set_ylabel('Born per year (blue)')\n",
    "    ax2.set_ylabel('Alive per year (red)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes well under 1 minute now.\n",
    "\n",
    "#Average life-weighted year-of birth for each name for each year:\n",
    "#DOES NOT CHECK WHETHER ANYONE IS STILL ALIVE IN X YEAR BEFORE \n",
    "#MATH, SO EXPECT DIVIDE BY ZERO ERRORS. NaNs are totally fine for\n",
    "#showing lack of alive people, but not pretty coding.\n",
    "\n",
    "#It is possible to do this with matrix multiplication. It would\n",
    "#take WAY longer to get running right than me just running this. \n",
    "\n",
    "agevec = np.arange(1880,2020).T\n",
    "\n",
    "aliveshape = namelife_M_full.shape\n",
    "namelife_M_yob = np.zeros(namelife_M_base.shape)\n",
    "namelife_sum = np.sum(namelife_M_full, axis=1)\n",
    "for rowind in np.arange(0,aliveshape[0]):\n",
    "    tempsum = namelife_sum[rowind,:]\n",
    "    namelife_M_yob[rowind,:] = np.matmul(agevec, namelife_M_full[rowind,:,:]) / \\\n",
    "                               namelife_sum[rowind,:]\n",
    "\n",
    "aliveshape = namelife_F_full.shape\n",
    "namelife_F_yob = np.zeros(namelife_F_base.shape)\n",
    "namelife_sum = np.sum(namelife_F_full, axis=1)\n",
    "for rowind in np.arange(0,aliveshape[0]):\n",
    "    namelife_F_yob[rowind,:] = np.matmul(agevec, namelife_F_full[rowind,:,:]) / \\\n",
    "                                namelife_sum[rowind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if showplot:\n",
    "    plt.plot(np.arange(1880, 2050), namelife_M_base[0,:])\n",
    "    plt.plot(np.arange(1880, 2050), \n",
    "             -1*(namelife_M_yob[0,:] - np.arange(1880,2050)))\n",
    "    plt.title(namelife_M_name[0])\n",
    "    plt.legend(['Number alive','Avg age'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General notes:\n",
    "\n",
    "#This would be a good place in the future to normalize by ESTIMATED\n",
    "#births in a given birth year. It's reasonable to only use SSA-named\n",
    "#births, but it's more likely to be a good representation of the \n",
    "#population if I scale name-set births up by the ratio of estimated\n",
    "#total births to recorded births.\n",
    "\n",
    "#How to do that: \"Calculate estimated births\" each year 1880+, multiply\n",
    "#pop of each name by the ratio of estimated total births to names\n",
    "#dataset births. \n",
    "#Options for calculating estimated births:\n",
    "#    -Derive number of births from the census (feasible)\n",
    "#    -Take the number of people alive of a given age, then scale the \n",
    "#     names alive to match (would be inaccurate due to immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The oldest male and female names in the US:\n",
    "\n",
    "#OPTIONAL VISUALIZATION\n",
    "\n",
    "if showplot:\n",
    "    namrange = 20 #how many to look at\n",
    "    print('\\n\\nThese are mostly birth names that appeared in few years, \\\n",
    "    and were held by few people.')\n",
    "\n",
    "    #Target year: 2020, aka location 1880+140\n",
    "    nameF_ageind = np.argsort(namelife_F_yob[:,140])\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    ax.plot(namelife_F_yob[nameF_ageind,140][0:namrange])\n",
    "    ax.set_xticks(range(namrange))\n",
    "    ax.set_xticklabels(namelife_F_name[nameF_ageind][0:namrange], rotation=90)\n",
    "    ax.set_title('Oldest female birth names')\n",
    "    ax.set_ylabel('Average year of birth')\n",
    "    plt.show()\n",
    "\n",
    "    nameM_ageind = np.argsort(namelife_M_yob[:,140])\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    ax.plot(namelife_M_yob[nameM_ageind,140][0:namrange])\n",
    "    ax.set_xticks(range(namrange))\n",
    "    ax.set_xticklabels(namelife_M_name[nameM_ageind][0:namrange], rotation=90)\n",
    "    ax.set_title('Oldest male birth names')\n",
    "    ax.set_ylabel('Average year of birth')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common names by age:\n",
    "\n",
    "#OPTIONAL VISUALIZATION\n",
    "\n",
    "if showplot:\n",
    "    fig = plt.figure(figsize=[12,10], facecolor='white')\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    plt.grid(b=True, axis='x', color='gray')\n",
    "    ax2 = plt.subplot(2,1,2)\n",
    "    plt.grid(b=True, axis='x', color='gray')\n",
    "\n",
    "\n",
    "    n_hold_M = 750000\n",
    "    n_hold_F = 500000\n",
    "    numscale = 6000\n",
    "\n",
    "    #Collect maximum number of holders alive for each name\n",
    "    namelife_F_max = namelife_F_base.max(1)\n",
    "    #Collect year index at which that occurred\n",
    "    namelife_F_max_yr = np.argmax(namelife_F_base, axis=1)\n",
    "    #Collect the avg. yob at the year index at which that occurred\n",
    "    namelife_F_max_yob = namelife_F_yob[np.arange(0,len(namelife_F_max)),namelife_F_max_yr]\n",
    "    #Collect the names for which that max was above the target threshold\n",
    "    chunk_F = namelife_F_max >= n_hold_F\n",
    "    #Select the YOB for data that met criteria, then collect year index that sorts it by YOB\n",
    "    sort_yr_F = np.argsort(namelife_F_yob[chunk_F,2020-1880])\n",
    "    #Select the number of individuals alive for data that met criteria, sort by above index\n",
    "    sort_yr_F_alive = namelife_F_base[chunk_F,2020-1880][sort_yr_F]\n",
    "    #Select the maximum number of individuals alive for data that met criteria, sort by above index\n",
    "    sort_yr_F_maxalive = namelife_F_max[chunk_F][sort_yr_F]\n",
    "    #Select the YOB at maximum number of individuals alive for data that met criteria, sort by above index\n",
    "    sort_yr_F_maxalive_yob = namelife_F_max_yob[chunk_F][sort_yr_F]\n",
    "\n",
    "    #Repeat above for M\n",
    "    namelife_M_max = namelife_M_base.max(1)\n",
    "    namelife_M_max_yr = np.argmax(namelife_M_base, axis=1)\n",
    "    namelife_M_max_yob = namelife_M_yob[np.arange(0,len(namelife_M_max)),namelife_M_max_yr]\n",
    "    chunk_M = namelife_M_max >= n_hold_M\n",
    "    sort_yr_M = np.argsort(namelife_M_yob[chunk_M,2020-1880])\n",
    "    sort_yr_M_alive = namelife_M_base[chunk_M,2020-1880][sort_yr_M]\n",
    "    sort_yr_M_maxalive = namelife_M_max[chunk_M][sort_yr_M]\n",
    "    sort_yr_M_maxalive_yob = namelife_M_max_yob[chunk_M][sort_yr_M]\n",
    "\n",
    "\n",
    "    xlims = [-2,max([sum(chunk_F), sum(chunk_M)])+1]\n",
    "    ylims = [1920, 2010]\n",
    "\n",
    "    n_hold = 300000\n",
    "\n",
    "    # lin_1 = ax.plot(alivelist_M_yob[chunk_M,2020-1880][sort_yr_M]+1880, color='blue', linewidth=1)\n",
    "    dot_1a = ax.scatter(x=np.arange(0,sum(chunk_M)),\n",
    "                        y=sort_yr_M_maxalive_yob,\n",
    "                        s=sort_yr_M_maxalive/numscale, \n",
    "                        color=[[.95,.95,.95]], edgecolor='tab:blue',zorder=5,label='Peak holders')\n",
    "    dot_1b = ax.scatter(x=np.arange(0,sum(chunk_M)),\n",
    "                        y=namelife_M_yob[chunk_M,2020-1880][sort_yr_M],\n",
    "                        s=sort_yr_M_alive/numscale,\n",
    "                        color='tab:blue', edgecolor='tab:blue',zorder=6,label='Current holders')\n",
    "    ax.set_xticks(range(sum(chunk_M)))\n",
    "    ax.set_xticklabels(namelife_M_name[chunk_M].iloc[sort_yr_M], rotation=90)\n",
    "    ax.set_ylim(ylims)\n",
    "    ax.set_xlim(xlims)\n",
    "    # ax.legend()\n",
    "\n",
    "\n",
    "    # lin_2 = ax2.plot(alivelist_F_yob[chunk_F,2020-1880][sort_yr_F]+1880, color='red', linewidth=1)\n",
    "    dot_1a = ax2.scatter(x=np.arange(0,sum(chunk_F)),\n",
    "                        y=sort_yr_F_maxalive_yob,\n",
    "                        s=sort_yr_F_maxalive/numscale, \n",
    "                        color=[[.95,.95,.95]], edgecolor='tab:red',zorder=5,label='Peak holders')\n",
    "    dot_1b = ax2.scatter(x=np.arange(0,sum(chunk_F)),\n",
    "                        y=namelife_F_yob[chunk_F,2020-1880][sort_yr_F],\n",
    "                        s=sort_yr_F_alive/numscale,\n",
    "                        color='tab:red', edgecolor='tab:red',zorder=6,label='Current holders')\n",
    "    ax2.set_xticks(range(sum(chunk_F)))\n",
    "    ax2.set_xticklabels(namelife_F_name[chunk_F].iloc[sort_yr_F], rotation=90)\n",
    "    ax2.set_ylim(ylims)\n",
    "    ax2.set_xlim(xlims)\n",
    "\n",
    "    # plt.grid(axis='y', color=0.9)\n",
    "\n",
    "    #proxy size labels:\n",
    "    legendpts = np.array([n_hold, n_hold*3, n_hold*10]).astype('int64')\n",
    "    scatter_for_legend = ax.scatter(x=[0,0,0], y=[0,0,0], \n",
    "                                    s=legendpts/numscale,\n",
    "                                    c=[[1,1,1,0],[1,1,1,0],[1,1,1,0]]\n",
    "                                   )\n",
    "    scatter_for_legend_2 = ax.scatter(x=[0], y=[0],\n",
    "                                      s=[1.5*legendpts[1]/numscale],\n",
    "                                      c=['tab:blue'])\n",
    "    scatter_for_legend_3 = ax.scatter(x=[0], y=[0],\n",
    "                                      s=[1.5*legendpts[1]/numscale],\n",
    "                                      c=[[.95,.95,.95]],\n",
    "                                      edgecolor=['tab:blue'])\n",
    "    handles, labels = scatter_for_legend.legend_elements(prop=\"sizes\", alpha=0.2)\n",
    "    handles_2, labels_2 = scatter_for_legend_2.legend_elements(prop=\"sizes\", alpha=1, color=[.25,.5,.85])\n",
    "    handles_3, labels_3 = scatter_for_legend_3.legend_elements(prop=\"sizes\", alpha=1, color=[.95,.95,.95])\n",
    "    # legend = ax.legend(handles=handles+[]+handles_2+handles_3,\n",
    "    #                    labels=['{0:,}'.format(m) for m in legendpts]+[]+['Living', 'Peak'],\n",
    "    #                    loc=\"center right\",\n",
    "    #                    handletextpad=3,\n",
    "    #                    labelspacing=1,\n",
    "    #                    framealpha=1,\n",
    "    #                    title=\"Number of holders\"\n",
    "    #                   )\n",
    "    legend = ax.legend(handles=handles+[]+handles_2+handles_3,\n",
    "                       labels=['{0:,}'.format(m) for m in legendpts]+[]+['Living', 'Peak'],\n",
    "                       loc=\"center right\",\n",
    "                       handletextpad=3,\n",
    "                       labelspacing=1,\n",
    "                       framealpha=1,\n",
    "                       title=\"Number of holders\"\n",
    "                      )\n",
    "\n",
    "    ax.set_title('All US birth names that reached more than {0:,}(M) or {1:,}(F) holders alive'.format(n_hold_M,n_hold_F))\n",
    "    ax.set_ylabel('Year of birth (M)')\n",
    "    ax2.set_ylabel('Year of birth (F)')\n",
    "    # fig.set_ylabel('Average year of birth for holders')\n",
    "    # ax2.set_ylabel('Average year of birth for holders living in 2020')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign names aliveness ranks by year\n",
    "aliverank_M_yr = np.full(namelife_M_base.shape, np.nan)\n",
    "for n in range(aliveshape[2]):\n",
    "    tempset = namelife_M_base[:,n] != 0\n",
    "    aliverank_M_yr[tempset,n] = np.argsort(-namelife_M_base[tempset,n])\n",
    "aliverank_F_yr = np.full(namelife_F_base.shape, np.nan)\n",
    "for n in range(aliveshape[2]):\n",
    "    tempset = namelife_F_base[:,n] != 0\n",
    "    aliverank_F_yr[tempset,n] = np.argsort(-namelife_F_base[tempset,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Future comparisons:\n",
    "\n",
    "#Stacked line plot of all high-count names (i.e. with >10,000 holders),\n",
    "#with color of the swath for that name varying w/ avg age at the x-axis\n",
    "#time. Large plot!\n",
    "\n",
    "#Geographic names (cities, states, rivers?)\n",
    "#Names and abbreviations (Lawrence and Larry, Christopher and Chris, \n",
    "#Robert and Bob/Rob, Richard and Rick/Dick, Harold and Harry, Thomas\n",
    "#and Tom/Thom, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin visualization code\n",
    "\n",
    "Below is the current visualization code. I am in the progress of optimizing this to work from a derived set of data, rather than calculating the full characteristic values for every name at every call. Currently, it uses a pair of matrices around (25000 | 40000) x 140 x 180 (~25k rows for male names, 40k rows for female names) which is a lot, but it could get by on the set of names/properties for each potential window and potential age. If I constrain the windows for year and age range to either 5 or 10 years and only save the top 100 names, then it's a (135+130) x (135+130) x 100 matrix for M and F each, cutting it down from >1 billion elements to ~15 million. \n",
    "\n",
    "From the user perspective, that would look like the user selecting a single year and a single age, then whether to use a window of 5 or 10 years around either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal colors: Plotly defaults, #EF553B (red), #636EFA (blue):\n",
    "#https://www.rapidtables.com/convert/color/rgb-to-hsv.html\n",
    "#HSV, red: 9deg, 75.3%, 93.7%\n",
    "#HSV, blue: 236deg, 60.4%, 98.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convenience function to plot the births for a given name\n",
    "\n",
    "def showname(name, sex):\n",
    "    subdf = names_df_trim[(names_df_trim['Name']==name) & (names_df_trim['Sex']==sex)]\n",
    "    plt.figure(figsize=[14,4])\n",
    "    plt.bar(subdf['Year'],subdf['Number'])\n",
    "    plt.xlim([1880,2020])\n",
    "    plt.title(f'Number of {name} ({sex}) born per year')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary data visualization function: WORD CLOUD\n",
    "\n",
    "def display_namecloud(namedict, agedict, target_range):\n",
    "    \n",
    "    #DISPLAY:\n",
    "    #TODO: Use viridis_r for individual-name age colormap\n",
    "    \n",
    "    #Make a circle mask\n",
    "    cscale = 1000\n",
    "    x, y = np.ogrid[:cscale, :cscale]\n",
    "    mask = (x - cscale/2) ** 2 + (y - cscale/2) ** 2 > (cscale/2-20) ** 2\n",
    "    mask = 255 * mask.astype(int)\n",
    "\n",
    "    built_cloud = WordCloud(background_color='white',\n",
    "                            repeat=False, \n",
    "                            width=800, \n",
    "                            height=500,\n",
    "                            max_font_size=250,\n",
    "                            mask=mask)\n",
    "    \n",
    "    wcloud = built_cloud.generate_from_frequencies(namedict)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=[10,10])\n",
    "    ax.axis('off')\n",
    "    #plt.imshow(wcloud, interpolation='bilinear')\n",
    "    #plt.show()\n",
    "    \n",
    "    if target_range[2]=='F':\n",
    "        hueval = 4\n",
    "    else:\n",
    "        hueval = 236\n",
    "    \n",
    "    #Alternative color pulled from:\n",
    "    #https://amueller.github.io/word_cloud/auto_examples/a_new_hope.html\n",
    "    #Also adapted from:\n",
    "    #https://stackoverflow.com/questions/47143461/python-wordcloud-color-by-term-frequency\n",
    "    \n",
    "    #CONSIDER ALTERNATIVE:\n",
    "    # colorsys.rgb_to_hls(200/255,100/255,200/255)\n",
    "    \n",
    "    #[120,360]\n",
    "    hue_lim = [100,260] #cyan to magenta, through blue. Mean 240, full blue.\n",
    "    sat_lim = [100,0]\n",
    "    lit_lim = [80,0]\n",
    "    age_lim = [target_range[1][0]-5,target_range[1][1]+5]\n",
    "    def hue_scaler(num):\n",
    "        #Simple linear transformation converts age from (n1)-5:(n2)+5 range to hsl_lim range\n",
    "        tgt_hue = hue_lim[0] + (num - age_lim[0])*(hue_lim[1] - hue_lim[0]) / (age_lim[1] - age_lim[0])\n",
    "        return tgt_hue\n",
    "    def sat_scaler(num):\n",
    "        #Scale from desat at old to colorful at young\n",
    "        tgt_sat = sat_lim[0] + (num - age_lim[0])*(sat_lim[1] - sat_lim[0]) / (age_lim[1] - age_lim[0])\n",
    "        #Scale saturation from middle (use 100,-100 for ex):\n",
    "        #         if tgt_sat < (sat_lim[1] + sat_lim[0])/2:\n",
    "        #             tgt_sat = sat_lim[1] - tgt_sat\n",
    "        return min([100,max([tgt_sat,0])])\n",
    "    def lit_scaler(num):\n",
    "        tgt_lit = lit_lim[0] + (num - age_lim[0])*(lit_lim[1] - lit_lim[0]) / (age_lim[1] - age_lim[0])\n",
    "        return min([100,max([tgt_lit,0])])\n",
    "    \n",
    "    def age_color_func(agedictionary):\n",
    "        def my_tf_color_func_inner(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "            temphue = hue_scaler(agedictionary[word])\n",
    "            tempsat = sat_scaler(agedictionary[word])\n",
    "            templit = lit_scaler(agedictionary[word])\n",
    "            #print(agedictionary[word], tempcolor, tempsat)\n",
    "            return f\"hsl({hueval}, {tempsat:.0f}%, {templit:.0f}%)\"\n",
    "        return my_tf_color_func_inner\n",
    "    \n",
    "    plt.imshow(wcloud.recolor(color_func=age_color_func(agedict), random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary data visualization function: SPARK(like) TREND LINES\n",
    "\n",
    "def display_sparklines(namerange_data, target_range, n_names=15):\n",
    "    \n",
    "    #Define convenience variables:\n",
    "    coh1 = target_range[0][0] - target_range[1][1] #Oldest cohort: first year minus oldest\n",
    "    coh2 = target_range[0][1] - target_range[1][0] #Youngest cohort: last year minus youngest\n",
    "    maxspecif = namerange_data[0].max() #Greatest name specificity, for scale\n",
    "    \n",
    "    #Initialize figure with subplots\n",
    "    fig = make_subplots(n_names+1, 1, \n",
    "                        shared_xaxes=True,\n",
    "                        specs = [[{}] for n in range(n_names+1)],\n",
    "                        vertical_spacing = 0,\n",
    "                       )\n",
    "    \n",
    "    #Set variables appropriately based on name sex\n",
    "    if target_range[2]=='M':\n",
    "        birthsource = namebirth_M\n",
    "        linecol = 'blue'\n",
    "        fillbase = 'rgb({0},{0},255)'\n",
    "    else:\n",
    "        birthsource = namebirth_F\n",
    "        linecol = 'red'\n",
    "        fillbase = 'rgb(255,{0},{0})'\n",
    "    \n",
    "    for i in np.arange(n_names)+1:\n",
    "        #Define single-name convenience variables \n",
    "        peakval = birthsource[namerange_data[5][-1*i],:].max()\n",
    "        yearscale = 1000*(target_range[0][1]-target_range[0][0]) #* (coh2 - coh1)\n",
    "        avg_alive = namerange_data[1][-1*i]/()\n",
    "        anno_text = '<b>' + namerange_data[4].iloc[-1*i] + '</b><br>' \\\n",
    "                    + '  Specificity: {0:.2f}<br>'.format(namerange_data[0][-1*i]/maxspecif) \\\n",
    "                    + '  Weight (~how many): {0:.0f}k'.format(namerange_data[1][-1*i]/yearscale)\n",
    "        \n",
    "        #Plotly plot generation\n",
    "        #Add bounding box showing target birth years\n",
    "        fig.add_trace(go.Scatter(x=[coh1, coh1, coh2, coh2],\n",
    "                                 y=[0,1.2*peakval,1.2*peakval,0],\n",
    "                                 fill=\"toself\",\n",
    "                                 mode='lines',\n",
    "                                 line={'color':'gray'}\n",
    "                                ), i+1, 1)\n",
    "        #Add trace of name birth history\n",
    "        fig.add_trace(go.Scatter(x=np.arange(1880,2050), \n",
    "                                 y=birthsource[namerange_data[5][-1*i],:],\n",
    "                                 mode='lines', \n",
    "                                 line={'color':linecol,\n",
    "                                       'width':max(5*peakval**0.75/100_000**0.75, 0.5)},\n",
    "                                 name=namerange_data[4].iloc[-1*i],\n",
    "                                 fill='tozeroy',\n",
    "                                 fillcolor=fillbase.format(255 - 255*peakval/100_000),\n",
    "                                ), i+1, 1)\n",
    "        #Add text stating specificity/weight specifically\n",
    "        fig.add_annotation(xref='x{}'.format(i+1), yref='y{}'.format(i+1),\n",
    "                           x=2020, y=0.52*peakval,\n",
    "                           text=anno_text,\n",
    "                           showarrow=False,\n",
    "                           align='left',\n",
    "                           xshift=100)\n",
    "        #Format subplot\n",
    "        fig.update_xaxes(showgrid=False,\n",
    "                         ticks='',\n",
    "                         showticklabels=False,\n",
    "                         range=[coh1-40, coh2+40],\n",
    "                         domain=[0,.6],\n",
    "                         zeroline=False,\n",
    "                        )\n",
    "        \n",
    "    fig.update_yaxes(showgrid=False, ticks='', showticklabels=False)\n",
    "    fig.add_annotation(xref='x1', yref='y1',\n",
    "                       x=np.mean([coh1, coh2]), y=0.5*peakval,\n",
    "                       text='<b>Names for ages {0}-{1} in {2}-{3} ({4}):</b>'\n",
    "                           .format(target_range[1][0],\n",
    "                                   target_range[1][1],\n",
    "                                   target_range[0][0],\n",
    "                                   target_range[0][1],\n",
    "                                   target_range[2]\n",
    "                                  ),\n",
    "                       showarrow=False,\n",
    "                       align='left',\n",
    "                       xshift=100)\n",
    "    fig.update_layout(template=None, \n",
    "                      height=n_names*(100*0.55), \n",
    "                      width=500,\n",
    "                      margin={'t':0,'b':0},\n",
    "                      showlegend=False,\n",
    "                     )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary data visualization function: GENERATE VALUES AND CALL VIS-DISPLAY FUNCTION\n",
    "\n",
    "def get_era_names(target_range, \n",
    "                  n_names=25, \n",
    "                  show_scores=False, \n",
    "                  justpopular=False, \n",
    "                  displayfunc=display_sparklines):\n",
    "\n",
    "    #This function generates the numbers used in the visualization function.\n",
    "    \n",
    "    #We want to look at the 20-30 range for each of 4 years, for example\n",
    "    #Year targets: n1-1880, n2-1880\n",
    "    year1 = target_range[0][0] - 1880\n",
    "    year2 = target_range[0][1] - 1880\n",
    "\n",
    "    coh1 = year1 - target_range[1][1] #Oldest cohort: first year minus oldest\n",
    "    coh2 = year2 - target_range[1][0] #Youngest cohort: last year minus youngest\n",
    "    \n",
    "    #POSSIBLE TUNE-UP TO INCREASE SPECIFICITY:\n",
    "    #Weight by how many years each cohort was in that range, not just by\n",
    "    #whether they were in the range at all.\n",
    "    \n",
    "    s = target_range[2]\n",
    "    \n",
    "    if s=='F':\n",
    "        #How many person-years were lived in the target:\n",
    "        totaltgt = np.sum(namelife_F_full[:, coh1:coh2, year1:year2], axis=(1,2))\n",
    "        #How many person-years were lived outside the target:\n",
    "        totalnon = np.sum(namelife_F_full, axis=(1,2))\n",
    "        #(Could narrow to comparing against the time window only to\n",
    "        #limit the impact of the same people being alive before/after,\n",
    "        #but this way makes sure people born after get counted in)\n",
    "    elif s=='M':\n",
    "        totaltgt = np.sum(namelife_M_full[:, coh1:coh2, year1:year2], axis=(1,2))\n",
    "        totalnon = np.sum(namelife_M_full, axis=(1,2))\n",
    "    else:\n",
    "        print('Gender options in social security name data are M or F')\n",
    "        return None\n",
    "        \n",
    "    #Calculate what percent of name's person-years happened in that window\n",
    "    name_specificity = totaltgt/totalnon\n",
    "    #Weight by total number with that name in the window\n",
    "    name_weight = totaltgt\n",
    "    #Calculate score: sqrt(number in window) times the specificity\n",
    "    name_score = 10 * np.sqrt(name_weight) * name_specificity**2\n",
    "    #Grab the average age for all nameholders during the window\n",
    "    if s=='F':\n",
    "        #Calculate mean year of the range, then subtract the mean\n",
    "        #year of birth (per name, :) across the range\n",
    "        name_ages = (1880*2+year1+year2)/2 - np.mean(namelife_F_yob[:,year1:year2+1], axis=1)\n",
    "    else:\n",
    "        name_ages = (1880*2+year1+year2)/2 - np.mean(namelife_M_yob[:,year1:year2+1], axis=1)\n",
    "    \n",
    "    #Grab the names themselves\n",
    "    if s=='F':\n",
    "        name_outs = namelife_F_name\n",
    "    else:\n",
    "        name_outs = namelife_M_name\n",
    "    \n",
    "    if justpopular:\n",
    "        #If you're just looking for most popular:\n",
    "        nameinds = np.argsort(name_weight)\n",
    "    else:\n",
    "        #If you're looking for most representative (DEFAULT):\n",
    "        nameinds = np.argsort(name_score) #defaults to asc\n",
    "\n",
    "    namedict = {}\n",
    "    agedict = {}\n",
    "    for n in range(n_names):\n",
    "        dictkey = name_outs[nameinds].iloc[-(n+1)]\n",
    "        if justpopular:\n",
    "            namedict[dictkey] = int(name_weight[nameinds][-(n+1)])\n",
    "        else:\n",
    "            namedict[dictkey] = int(name_score[nameinds][-(n+1)])\n",
    "        agedict[dictkey] = name_ages[nameinds][-(n+1)] \n",
    "        \n",
    "    namerange_data = (name_specificity[nameinds],\n",
    "                      name_weight[nameinds],\n",
    "                      name_score[nameinds],\n",
    "                      name_ages[nameinds],\n",
    "                      name_outs[nameinds],\n",
    "                      nameinds\n",
    "                     )\n",
    "\n",
    "    #Visualization subfunction: WORDCLOUD    \n",
    "    display_namecloud(namedict, agedict, target_range)\n",
    "    display_sparklines(namerange_data, target_range, n_names=n_names)\n",
    "    \n",
    "    #Visualization subfunction: DEFAULT SPARKLINE\n",
    "    #displayfunc(namerange_data)\n",
    "\n",
    "    if show_scores:\n",
    "        print('Specificity, weight, score, avg. age, name:')\n",
    "        for item in zip(np.round(name_specificity[nameinds][::-1][:n_names],3),\n",
    "                 np.round(name_score[nameinds][::-1][:n_names],2),\n",
    "                 np.round(name_ages[nameinds][::-1][:n_names],2),\n",
    "                 name_outs[nameinds][::-1][:n_names]):\n",
    "            print(item)\n",
    "    \n",
    "    return namerange_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Characteristic names: \n",
    "\n",
    "Based on a given era and a given age range, can we find names that feel right? \n",
    "\n",
    "We're looking for names that are at least fairly common, so you'd expect to see them, but also names that are *uncommon* outside the target range.\n",
    "\n",
    "The second visualization is close to deployable, Plotly is simple to get html for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_range = [[1995, 2000], [20,25], 'M'] #years, age, sex\n",
    "\n",
    "namerange_data = get_era_names(target_range, n_names=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL VISUALIZATION: Specificity vs. number alive. Good for\n",
    "#eyeballing how the score algorithm balances between the two.\n",
    "\n",
    "#Could use a tune-up. Size could show number alive, for example.\n",
    "if showplot:\n",
    "    ann_num = 10\n",
    "    plt.figure(figsize=[10,6])\n",
    "    plt.scatter(namerange_data[0], namerange_data[1], c=namerange_data[2])\n",
    "    for n in range(-1*ann_num,0):\n",
    "        plt.annotate(text=namerange_data[4].iloc[n], \n",
    "                     xy=(namerange_data[0][n], namerange_data[1][n]),\n",
    "                     xytext=(2,2),\n",
    "                     textcoords='offset points',\n",
    "                     rotation=30\n",
    "                    )\n",
    "    plt.xlabel(\"Specificity: Fraction of name's years that were lived in range\")\n",
    "    plt.ylabel('Weight: Number of name-years lived')\n",
    "    plt.ylim([-200000, 1.15*max(namerange_data[1])])\n",
    "    plt.box()\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.ax.set_ylabel('Characteristicness Score')\n",
    "    plt.title('Years: {}, Ages: {}, Sex: {}'.format(target_range[0], target_range[1], target_range[2]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Name clustering and prediction:\n",
    "\n",
    "In short, based on the properties of the names we know of, predict whether a name will increase or decrease in a given year. Currently on version 1, a simple Ridge. Below are notes on how the clustering will work and future improvements and projects that would improve the function. \n",
    "\n",
    "\n",
    "### Notes on name similarity clustering:\n",
    "\n",
    "**The goal:** Cluster names based on how similarly they are spelled or pronounced. \n",
    "\n",
    "Let's start with the spelling, since the pronunciation similarity has additional layers of complexity. \n",
    "\n",
    "Let's start with the spelling; I'll be using Levenshtein distance as a starting point, which is a very convenient measure of how similar two words are to one another, also known as the \"edit distance\". It doesn't take into account the fact that there some letters are more similar to one another than others (i.e. \"a\" is drastically more like \"e\" than \"q\"), but it's a great start, and I can update the distance algorithm as I go. (The more complex letter similarity is probably most appropriately handled in the pronunciation clustering, anyway.)\n",
    "\n",
    "So! The core tech here is going to be **affinity propagation**. For that, we need similarity measures between each pair of words, which to reiterate will be the Levenshtein distance. \n",
    "\n",
    "Minor problem: Using the current set of names, the number of comparisons is big. The **total number of comparisons is O(nÂ²)**, which is not good when you have a set of 100,000 names. I'm going to have to prune this. I'll be focusing on the most relevant names, i.e. those which met a certain threshold of use. One possible threshold: maximum number of people who have had that name at any time. You can see in the plot below what this ends up looking like. You can also see that the name distribution follows a pretty clear pattern. \n",
    "\n",
    "In short, to keep our number of comparisons reasonable, **we'll limit it based on peak holders alive.** We can calculate which clusters additional names would fit in pretty easily, but for the clustering itself, this may be close to the outer limits of what we can practically do with affinity propagation using this precomputed distance.\n",
    "\n",
    "**PROBABLE FUTURE ADDITION:**  \n",
    "Pre-group names with IDENTICAL predicted pronunciations, THEN get the top set. That will, for example, catch the literally 40 different ways of spelling \"Abigail\" under a single umbrella, or the \"Brayden/Breighdon\" etc. name clan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting pronunciations for clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRONUNCIATION:\n",
    "\n",
    "#Calculate word pronunciation based on the Datamuse API response.\n",
    "#Works fairly well, though some names such as Abrielle get lumped\n",
    "#together with Gabrielle in pronunciation, as it assumes it has a\n",
    "#leading G. \n",
    "#\n",
    "#https://api.datamuse.com/words?sl=mackenzie&md=r\n",
    "#\n",
    "#This API call returns both a word's pronunciation and words that\n",
    "#sound like it.\n",
    "\n",
    "import string\n",
    "from ediblepickle import checkpoint\n",
    "\n",
    "#Generate checkpoint data for each function call, which goes in\n",
    "#the \"name_pronunciation_storage\" directory. Note that this will\n",
    "#run identical names twice, once each for male or female, but the\n",
    "#Datamuse API does not use this information. Could be optimized, \n",
    "#but the load process is very quick. \n",
    "\n",
    "@checkpoint(key=string.Template('name_pronunc_{0}_{1}.pkl'), work_dir='name_pronunciation_storage', refresh=False)\n",
    "def get_name_pronunc(name, sex):\n",
    "    #Define core API call, which works great with the word itself at the end\n",
    "    baseapi = 'https://api.datamuse.com/words?md=r&ipa=1&sl='\n",
    "    #Get page response for api+name.\n",
    "    \n",
    "    #V1: Use futuressessions\n",
    "    #REQUIRES THAT A FUTURESSESSION BE ACTIVE! \"session = FuturesSession()\"\n",
    "    resp = session.get(baseapi + name)\n",
    "    #Pull out just the content\n",
    "    pagecont = resp.result().content\n",
    "    pagetext = resp.result().text\n",
    "    \n",
    "    #V2: Use a standard request\n",
    "    #resp = requests.get(baseapi + name)\n",
    "    #pagecont = resp.content\n",
    "    #pagetext = resp.text\n",
    "    \n",
    "    #Fairly messy way to do this, probably ought to use a class\n",
    "    sylstart = pagetext.find('numSyllables')\n",
    "    syl = int(pagetext[sylstart+14])\n",
    "    arpastart = pagetext.find('pron:')\n",
    "    arpastop = pagetext.find('\"', arpastart)\n",
    "    arpa = pagetext[arpastart+5:arpastop]\n",
    "    ipastart = pagetext.find('ipa_pron:')\n",
    "    ipastop = pagetext.find('\"', ipastart)\n",
    "    ipa = pagetext[ipastart+9:ipastop]\n",
    "    \n",
    "    #print(ipa)\n",
    "    pronunc = []\n",
    "    pronunc.append(syl)\n",
    "    pronunc.append(arpa)\n",
    "    pronunc.append(ipa)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "    return pronunc, pagetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "redo_pronunc = False\n",
    "#Should functionize this. Takes about a minute to run through\n",
    "#the whole thing IF you're loading again, since the name data\n",
    "#loads from checkpoint. If the data is not checkpointed yet,\n",
    "#the initial collection takes multiple hours. \n",
    "\n",
    "if redo_pronunc:\n",
    "    session = FuturesSession()\n",
    "    all_pronunciations_F = {}\n",
    "    all_pagecont_F = []\n",
    "\n",
    "    for name in namelife_F_name:\n",
    "        pronunc, pagecont = get_name_pronunc(name, 'F')\n",
    "        all_pronunciations_F[name] = pronunc\n",
    "        all_pagecont_F.append(pagecont)\n",
    "        \n",
    "    with open('all_pronunciations_F.pkl', 'wb') as fname:\n",
    "        pickle.dump(all_pronunciations_F, fname)\n",
    "        \n",
    "else:\n",
    "    with open('all_pronunciations_F.pkl', 'rb') as fname:\n",
    "        all_pronunciations_F = pickle.load(fname)\n",
    "\n",
    "if redo_pronunc:\n",
    "    session = FuturesSession()\n",
    "    all_pronunciations_M = {}\n",
    "    all_pagecont_M = []\n",
    "\n",
    "    for name in namelife_M_name:\n",
    "        pronunc, pagecont = get_name_pronunc(name, 'M')\n",
    "        all_pronunciations_M[name] = pronunc\n",
    "        all_pagecont_M.append(pagecont)\n",
    "        \n",
    "    with open('all_pronunciations_M.pkl', 'wb') as fname:\n",
    "        pickle.dump(all_pronunciations_M, fname)\n",
    "        \n",
    "else:\n",
    "    with open('all_pronunciations_M.pkl', 'rb') as fname:\n",
    "        all_pronunciations_M = pickle.load(fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD CONTENT: (works, but not functionized)\n",
    "\n",
    "# session = FuturesSession()\n",
    "# baseapi = 'https://api.datamuse.com/words?md=r&sl='\n",
    "#\n",
    "# temptime = time.time()\n",
    "# all_responses_F = []\n",
    "# all_pronunciations_F = []\n",
    "# for name in alivelist_F_name[:1000]:\n",
    "#     resp = session.get(baseapi + name)\n",
    "#     pagecont = resp.result().content\n",
    "#     time.sleep(0.05)\n",
    "#     all_responses_F.append(pagecont)\n",
    "#\n",
    "#     #find the pronunciation tag itself:\n",
    "#     startpt = pagecont.find(b'tags')\n",
    "#     endpt = pagecont.find(b']', startpt)\n",
    "#     all_pronunciations_F.append(pagecont[startpt+8:endpt])\n",
    "#   \n",
    "# all_pronunciations_M = []\n",
    "# for name in alivelist_M_name[:1000]:\n",
    "#     resp = session.get(baseapi + name)\n",
    "#     time.sleep(0.05)\n",
    "#     all_pronunciations_M.append(resp)\n",
    "#\n",
    "# print(time.time() - temptime)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     #saved earlier, copied here for reference convenience\n",
    "\n",
    "#     names_df_trim = pd.read_pickle('names_df_trim.pkl')\n",
    "#     #saved above if refresh\n",
    "#     namelife_F_full = np.load('namelife_F_full.npy')\n",
    "#     namelife_F_base = np.load('namelife_F_base.npy')\n",
    "#     namelife_F_name = pd.read_pickle('namelife_F_name.pkl')\n",
    "#     namebirth_F = np.load('namebirth_F.npy')\n",
    "#     namelife_M_full = np.load('namelife_M_full.npy')\n",
    "#     namelife_M_base = np.load('namelife_M_base.npy')\n",
    "#     namelife_M_name = pd.read_pickle('namelife_M_name.pkl')\n",
    "#     namebirth_M = np.load('namebirth_M.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the pronunciations are very reasonable in most cases, though they do look like they sometimes default to a known pronunciation when words are spelled similarly enough (for example, some of the \"Abrielle\"-style names actually get a \"Gabrielle\" pronunciation). There's a \"Lebronjames\" in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan subset of data for clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Compare how many names you keep depending on your cutoff threshold\n",
    "#for whether a name has enough holders (requires hand-tuning, should\n",
    "#include the code from the earlier )\n",
    "\n",
    "commons_list_M = []\n",
    "commons_list_F = []\n",
    "#Test thresholds of 1-100000, using 20 log steps\n",
    "testvals = np.logspace(0,5,20)\n",
    "for n in testvals:\n",
    "    commons_list_M.append(sum(namelife_M_base.max(1) > n))\n",
    "    commons_list_F.append(sum(namelife_F_base.max(1) > n))\n",
    "    \n",
    "plt.loglog(testvals, commons_list_M, 'b', testvals, commons_list_F, 'r')\n",
    "plt.xlabel('Minimum people of a given name alive at one time for inclusion')\n",
    "plt.ylabel('Number of total names')\n",
    "plt.show()\n",
    "plt.plot(testvals, commons_list_M, 'b', testvals, commons_list_F, 'r')\n",
    "plt.xlabel('Minimum people of a given name alive at one time for inclusion')\n",
    "plt.ylabel('Number of total names')\n",
    "plt.ylim([0,10000])\n",
    "plt.show()\n",
    "\n",
    "#A couple specific test values:\n",
    "thresh1 = 50\n",
    "thresh2 = 5000\n",
    "commons_M_1 = namelife_M_base.max(1) > thresh1\n",
    "commons_F_1 = namelife_F_base.max(1) > thresh1\n",
    "commons_M_2 = namelife_M_base.max(1) > thresh2\n",
    "commons_F_2 = namelife_F_base.max(1) > thresh2\n",
    "\n",
    "#Total names:\n",
    "print('note: set is now lightly trimmed coming in, so small thresholds do little')\n",
    "print('Total male names >{}: '.format(thresh1) + str(sum(commons_M_1)))\n",
    "print('Total female names >{}: '.format(thresh1) + str(sum(commons_F_1)))\n",
    "print('Total male names >{}: '.format(thresh2) + str(sum(commons_M_2)))\n",
    "print('Total female names >{}: '.format(thresh2) + str(sum(commons_F_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name test sets:\n",
    "\n",
    "thresh=5000\n",
    "\n",
    "nametest_M_inds = namelife_M_base.max(1) > thresh\n",
    "\n",
    "nametest_M_name = namelife_M_name[nametest_M_inds]\n",
    "nametest_M_pron = pd.Series([all_pronunciations_M[tempname][2] for tempname in nametest_M_name])\n",
    "nametest_M_pronA = pd.Series([all_pronunciations_M[tempname][1] for tempname in nametest_M_name])\n",
    "nametest_M_num = namelife_M_base[nametest_M_inds].max(1)\n",
    "nametest_M_birth = namebirth_M[nametest_M_inds,:]\n",
    "\n",
    "nametest_F_inds = namelife_F_base.max(1) > thresh\n",
    "nametest_F_name = namelife_F_name[nametest_F_inds]\n",
    "nametest_F_pron = pd.Series([all_pronunciations_F[tempname][2] for tempname in nametest_F_name])\n",
    "nametest_F_pronA = pd.Series([all_pronunciations_F[tempname][1] for tempname in nametest_F_name])\n",
    "nametest_F_num = namelife_F_base[nametest_F_inds].max(1)\n",
    "nametest_F_birth = namebirth_F[nametest_F_inds,:]\n",
    "\n",
    "#Raw name test set:\n",
    "nametest_S_name = np.concatenate([nametest_M_name, nametest_F_name])\n",
    "nametest_S_pron = np.concatenate([nametest_M_pron, nametest_F_pron])\n",
    "nametest_S_pronA = np.concatenate([nametest_M_pronA, nametest_F_pronA])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name fitting thoughts:\n",
    "\n",
    "First up, basically time course fitting like this should maybe be done using a RNN (Recurrent Neural Network) in Tensor Flow, it's implemented well with Keras apparently. Second up, how do we get to a large set of predicted outcomes from a similarly-large set of data?\n",
    "\n",
    "**1. What is the target outcome?**\n",
    "\n",
    "First option: Predict how popular a given name *should* be in a given year, based on popularity of other names and the name's own popularity in the last, say, five years. Why is this hard? Large number of names. There's a high likelihood of overfitting, among other things. \n",
    "\n",
    "Second option: Much simpler, just use the time course correlation between different names over, say, the last 40 years to determine their similarity today. This allows grouping. \n",
    "\n",
    "Additional option: Predict name frequency in a given year based on number of each other name currently alive, and their ages (or age structure). This is likely to actually be possible and interesting. \n",
    "\n",
    "**2. What to compare?**\n",
    "\n",
    "Options: Graph distance (Affinity Propagation, Spectral Clustering, Markov Clustering (MCL)?b)\n",
    "\n",
    "More detailed spelling comparison: Bonus points for contiguous runs of letters that do not change? Shared syllable-equivalents?\n",
    "\n",
    "Simple option: Semi-manual clustering. Names with identical pronunciation are automatically clustered, along with names with no more than one letter difference. \n",
    "\n",
    "\n",
    "**ASSORTED ADDITIONAL FEATURES:**  \n",
    "-Letters (bag of words style) (include capitals for catching initial letters!)  \n",
    "-Letter bigrams bag  \n",
    "-Pronunciation bag  \n",
    "-Pronunciation bigrams bag  \n",
    "\n",
    "-Next-letter prediction? I've seen this done, could be interesting. \n",
    "\n",
    "-Clustering time-series!\n",
    "https://www.researchgate.net/publication/4756297_Model-Based_Clustering_of_Multiple_Time_Series.\n",
    "Looks like it would be a bit of a pain to implement, but potentially really interesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "#Using the distance module for Levenshtein because sure\n",
    "import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nametest_S_name.shape, nametest_M_name.shape, nametest_F_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#This is a costly part. Takes about 7 minutes at a threshold of >5000 alive.\n",
    "\n",
    "#Borrowing from code:\n",
    "#https://stats.stackexchange.com/questions/123060/clustering-a-long-list-of-strings-words-into-similarity-groups\n",
    "#https://stackoverflow.com/questions/21511801/text-clustering-with-levenshtein-distances\n",
    "\n",
    "#pronA takes noticeably more time, since the strings are longer. \n",
    "\n",
    "lev_similarity_name = 1-np.array([[distance.nlevenshtein(nameM1,nameM2) for nameM1 in nametest_M_name] for nameM2 in nametest_M_name])\n",
    "lev_similarity_pron = 1-np.array([[distance.nlevenshtein(nameM1,nameM2) for nameM1 in nametest_M_pron] for nameM2 in nametest_M_pron])\n",
    "lev_similarity_pronA = 1-np.array([[distance.nlevenshtein(nameM1,nameM2) for nameM1 in nametest_M_pronA] for nameM2 in nametest_M_pronA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Not very costly. Once the similarity matrices are built, the cluster fitting\n",
    "#only takes a few seconds.\n",
    "\n",
    "#Set up a near-default affinity propagation instance\n",
    "affprop_M_name = AffinityPropagation(affinity='precomputed', preference=0, random_state=None)\n",
    "affprop_M_pron = AffinityPropagation(affinity='precomputed', preference=0, random_state=None)\n",
    "# affprop_M_pronA = AffinityPropagation(affinity='precomputed', preference=0) #Basically doesn't converge.\n",
    "\n",
    "affprop_M_name.fit(lev_similarity_name)\n",
    "affprop_M_pron.fit(lev_similarity_pron)\n",
    "# affprop_M_pronA.fit(lev_similarity_pronA)\n",
    "\n",
    "#The ARPABET pronunciation does not really converge on its own,\n",
    "#looks like it may not cluster very neatly on its own. \n",
    "\n",
    "#However, as an additional perspective on pronunciation, it's \n",
    "#probably still useful to add. In practice, the clusters do \n",
    "#seem to make more sense when it's included.\n",
    "\n",
    "#After fiddling with it a little, the most \"sensible\" clusters seem\n",
    "#to come from weighting the pronunciations at about .75 apiece, so\n",
    "#they make up more than half the total but less than double the\n",
    "#spelling weight. \n",
    "\n",
    "affprop_M_triple = AffinityPropagation(affinity='precomputed', preference=0.1, random_state=None)\n",
    "affprop_M_triple.fit(lev_similarity_name + 0.75*lev_similarity_pron + 0.75*lev_similarity_pronA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affinity propagation notes:\n",
    "\n",
    "The IPA pronunciation-difference Levenshtein similarity is not very good. It would probably be better to implement the ARPABET pronunciations, since that model contains useful information about sound similarity based on just the characters present in each sound's codeb. That will take a lot more work, since it'd be effectively two-level Levenshtein, I'd have to think about how to reconcile that. \n",
    "\n",
    "One way would be instead of \"one-character substitution\" you could implement \"character difference\". For example, the \"A\" in \"Adrian\" is \"EY1\", where the \"A\" in \"Andrew\" is \"AE1\". That would be a Levenshtein distance of 2 (one subtraction to E1, one addition to AE1, or other ways to get there). Okay so that's not necessarily the greatest, since it would give a normalized Levenshtein of 0.66 (2/3 difference) when the sounds are reasonably similar, but it's still better than the complete difference that IPA would give. Character addition/subtraction would still be a binary scale. \n",
    "\n",
    "(Tried to find a measure of how \"far apart\" different vowels are, but the data isn't included in a paper that does what looks like a good job of it: http://www.martijnwieling.nl/files/Interspeech2013-FTetal.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_affprop = affprop_M_triple\n",
    "tgt_namebirth = namebirth_M\n",
    "\n",
    "print(len(np.unique(tgt_affprop.labels_)))\n",
    "for cluster_id in np.unique(tgt_affprop.labels_):\n",
    "    exemplar = nametest_M_name.iloc[tgt_affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(nametest_M_name.iloc[np.nonzero(tgt_affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    \n",
    "#Functionally, some of this is tracking what sounds are popular, which is fine.\n",
    "\n",
    "#The clusters are more or less sensible. Some names don't fit in very well; may\n",
    "#want to consider once again trying to hunt down an algorithm that's more willing\n",
    "#to leave solo clusters, because some of these names just flat don't fit very well\n",
    "#with any other name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example name comparison\n",
    "\n",
    "testname1 = 'Andrew'\n",
    "testname2 = 'Adrian'\n",
    "\n",
    "print('Example name comparison with eye toward comparing sounds later:\\n')\n",
    "print(testname1)\n",
    "print(all_pronunciations_M[testname1])\n",
    "print(testname2)\n",
    "print(all_pronunciations_M[testname2])\n",
    "print('\\nSpelling Lev:')\n",
    "print(distance.nlevenshtein(testname1,testname2))\n",
    "print('IPA Lev:')\n",
    "print(distance.nlevenshtein(all_pronunciations_M[testname1][2], all_pronunciations_M[testname2][2]))\n",
    "print('ARPABET Lev:')\n",
    "print(distance.nlevenshtein(all_pronunciations_M[testname1][1], all_pronunciations_M[testname2][1]))\n",
    "\n",
    "sound1 = all_pronunciations_M[testname1][1].split()[0]\n",
    "sound2 = all_pronunciations_M[testname2][1].split()[0]\n",
    "print('ARPABET first sound Lev: ' + sound1 + ' vs ' + sound2)\n",
    "print(distance.nlevenshtein(sound1, sound2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate cluster timecourses:\n",
    "\n",
    "cluster_courses = np.zeros([len(np.unique(tgt_affprop.labels_)),nametest_M_birth.shape[1]])\n",
    "for i,cluster_id in enumerate(np.unique(tgt_affprop.labels_)):\n",
    "    cc = nametest_M_birth[tgt_affprop.labels_==cluster_id,:]\n",
    "    cluster_courses[i,:] = np.sum(cc, axis=0)\n",
    "    \n",
    "#Normalize to year total -> fraction of year's births in X cluster\n",
    "cc_norm = cluster_courses/np.sum(cluster_courses, axis=0)\n",
    "#Normalize from fraction of year to standard scale\n",
    "cc_norm2 = (cc_norm - np.mean(cc_norm, axis=1).reshape(-1,1))/np.std(cc_norm, axis=1).reshape(-1,1)\n",
    "\n",
    "# plt.plot(cc_norm.T)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Fit first-10 with visual verification:\n",
    "name_linmodels = []\n",
    "\n",
    "#The scores are often terrible, because it's comparing versus\n",
    "#using the mean of the KNOWN data, but visually inspecting the\n",
    "#first 30 or so it's actually doing a reasonable job in most\n",
    "#cases. \n",
    "for n in range(30):\n",
    "    lin_mod = Ridge(alpha=0.01)\n",
    "\n",
    "    y_examp = nametest_M_birth[n,:].T\n",
    "    X_examp = np.hstack([cc_norm2.T, np.arange(1880,2020).reshape(-1,1)])\n",
    "\n",
    "    #Random split:\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_examp, y_examp, test_size=0.25)\n",
    "    #Year split:\n",
    "    X_train, X_test, y_train, y_test = X_examp[:-5], X_examp[-5:], y_examp[:-5], y_examp[-5:]\n",
    "\n",
    "    lin_mod.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lin_mod.predict(X_test)\n",
    "    \n",
    "    print(nametest_M_name.iloc[n])\n",
    "    print(lin_mod.score(X_test, y_test))\n",
    "    \n",
    "    plt.plot(X_train[:,-1], y_train, 'b.')\n",
    "    plt.plot(X_test[:,-1], y_test, 'b+')\n",
    "    plt.plot(X_test[:,-1], y_pred, 'rx')\n",
    "    plt.legend(['Train','Test-actual','Test-pred'])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_examp = np.hstack([cc_norm2.T, np.arange(1880,2020).reshape(-1,1)])\n",
    "\n",
    "#Re-fit using entire set prior to the last 5 years. Repeat this multiple\n",
    "#times using random sets of the data each time. \n",
    "\n",
    "n_refits = 20\n",
    "name_linmodels = []\n",
    "# name_linmodels = np.zeros([nametest_M_birth.shape[0], X_examp.shape[1], n_refits])\n",
    "name_linmodels_dic = {}\n",
    "test_scores = []\n",
    "\n",
    "for n in range(len(nametest_M_name)):\n",
    "    y_examp = nametest_M_birth[n,:].T\n",
    "\n",
    "    lin_mod = RidgeCV(cv=n_refits, alphas=[.1])\n",
    "\n",
    "    #Primitive bootstrap: Fit on partial data repeatedly, then average. \n",
    "    #Year split:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_examp[:-5], y_examp[:-5], test_size=0.1)\n",
    "    lin_mod.fit(X_train, y_train)\n",
    "\n",
    "    name_linmodels.append(lin_mod)\n",
    "    #name_linmodels[m, :, n] = lin_mod.coef_\n",
    "\n",
    "    test_score = lin_mod.score(X_examp[-5:], y_examp[-5:])\n",
    "    test_scores.append(test_score)\n",
    "    print('{0}: train {1:.3f}, test {2:.3f}, val {3:.3f} ({4})'.format(nametest_M_name.iloc[n],\n",
    "                                                                       lin_mod.score(X_train, y_train),\n",
    "                                                                       lin_mod.score(X_test, y_test),\n",
    "                                                                       test_score,\n",
    "                                                                       n\n",
    "                                                                      ))\n",
    "    y_pred_st = lin_mod.predict(X_test)\n",
    "    y_pred_fu = lin_mod.predict(X_examp[-5:])\n",
    "    if n%20==0:\n",
    "        plt.plot(X_train[:,-1], y_train, 'b.')\n",
    "        plt.plot(X_test[:,-1], y_test, 'b+')\n",
    "        plt.plot(X_test[:,-1], y_pred_st, 'rx')\n",
    "        plt.plot(X_examp[-5:,-1], y_examp[-5:], 'c+')\n",
    "        plt.plot(X_examp[-5:,-1], y_pred_fu[-5:], 'mx')\n",
    "        plt.legend(['Train','Test-actual','Test-pred','Val.-actual','Val.-pred'])\n",
    "        plt.show()\n",
    "    name_linmodels_dic[nametest_M_name.iloc[n]] = lin_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the interspersed test plots, while the scores are very poor, that's because the scoring compares them against the *known* mean value of the last five years. They're actually reasonable predictions given the past history of the name in most cases! Even hitting a score of zero (performing as well as the actual mean of the last five years) is quite good. There are a few notable problems, though. For example, the estimates are often overly-optimistic for names that are currently in a long-term low. It really thought Bert and Cletus were going to come back, for example. Similarly, it sometimes heads significantly negative, which is an obvious problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(name_linmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name trend detection preliminary conclusion:\n",
    "\n",
    "With careful variable preprocessing, a simple linear model does a surprisingly good job of tracking name trends without drastic overfitting. I'd need to extend this significantly to allow next-year prediction (i.e. generating projected name-cluster data), but it's a clear sign that we can reasonably track some trends for relatively popular names. To use the prior model for predictions, we'd have to generate synthetic cluster-trend data, which is feasible. For example, let's plug what we have into a time-series model like ARIMA, which uses the time course of the data to project its future trajectory. This is like to do reasonably well for near-future projection, and we can use that projection to generate the synthetic cluster trends necessary for the linear model to predict. Of course, we could also use the ARIMA directly on each name, but that would take hours for the full set of names compared to seconds for the linear model, and would also discard the information we can calculate about how names trend together. \n",
    "\n",
    "(Aside: This might actually be a pretty good spot for a multidimensional Kalman filter, which would use information from each timecourse variable quite nicely. I've been waiting for an excuse to get one of those going, might finally be the time!)  \n",
    "(Also, there are 0 babies named Arima in the names database, which is honestly a little surprising.)\n",
    "\n",
    "## ARIMA name timecourse projection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names_df_trim[names_df_trim['Name'] == 'Arima'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "\n",
    "#Handy reference for transforms:\n",
    "#https://alkaline-ml.com/pmdarima/usecases/sun-spots.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Testing this on most recent 5 years, so all but last 5 in training set\n",
    "tsize = 135\n",
    "\n",
    "num_clust = cc_norm2.shape[0]\n",
    "\n",
    "arima_mods = []\n",
    "for n in range(num_clust):\n",
    "    y = cc_norm2[n,:]\n",
    "    y_train, y_test = train_test_split(y, train_size=tsize)\n",
    "\n",
    "    arima_mod = pm.auto_arima(y_train)\n",
    "\n",
    "    y_pred = arima_mod.predict(y_test.shape[0])  # predict N steps into the future\n",
    "\n",
    "    ## Visualize the forecasts (blue=train, green=forecasts)\n",
    "    #x = np.arange(y.shape[0])\n",
    "    #plt.plot(x[:tsize], y_train, c='blue')\n",
    "    #plt.plot(x[tsize:], y[tsize:], 'b--')\n",
    "    #plt.plot(x[tsize:], y_pred, c='green')\n",
    "    #plt.show()\n",
    "    \n",
    "    arima_mods.append(arima_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the predicted-cluster-trend matrix\n",
    "\n",
    "y_pred_arima = np.zeros([num_clust, 5])\n",
    "for n in range(num_clust):\n",
    "    y_pred_arima[n,:] = arima_mods[n].predict(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual name trend predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#STEP 1: REDO THE ARIMAS WITH ALL YEARS\n",
    "\n",
    "tsize = 140\n",
    "\n",
    "num_clust = cc_norm2.shape[0]\n",
    "\n",
    "arima_mods = []\n",
    "for n in range(num_clust):\n",
    "    y = cc_norm2[n,:]\n",
    "    #y_train, y_test = train_test_split(y, train_size=tsize)\n",
    "    y_train = y\n",
    "\n",
    "    arima_mod = pm.auto_arima(y_train)\n",
    "\n",
    "    y_pred = arima_mod.predict(5)  # predict N steps into the future\n",
    "\n",
    "    ## Visualize the forecasts (blue=train, green=forecasts)\n",
    "    #x = np.arange(y.shape[0])\n",
    "    #plt.plot(x[:tsize], y_train, c='blue')\n",
    "    #plt.plot(x[tsize:], y[tsize:], 'b--')\n",
    "    #plt.plot(x[tsize:], y_pred, c='green')\n",
    "    #plt.show()\n",
    "    \n",
    "    arima_mods.append(arima_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_arima.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the predicted-cluster-trend matrix\n",
    "\n",
    "y_pred_arima = np.zeros([num_clust, 5])\n",
    "for n in range(num_clust):\n",
    "    y_pred_arima[n,:] = arima_mods[n].predict(5)\n",
    "\n",
    "#Add the pre-predicted value to the stack to convert the 5\n",
    "#predicted values to a change (6 vals -> 5 differences)\n",
    "baseval = cc_norm2[:,-1]\n",
    "y_pred_arima = np.hstack([baseval.reshape(-1,1),y_pred_arima])\n",
    "#Calculate the X-predicted in the difference format:\n",
    "X_pred_arima = np.hstack([np.diff(y_pred_arima).T, np.arange(2020,2025).reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_arima.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_examp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#STEP 2: Redo the linear fits to include the complete dataset.\n",
    "\n",
    "#1. Convert to CHANGE by year (solves most of the negatives problems)\n",
    "temp_delta = np.diff(cc_norm2) #yes, correct axis (1) is default\n",
    "temp_bdelta = np.diff(nametest_M_birth) #^same\n",
    "\n",
    "#2. Start in 1881, since that's the first year we can do diff for\n",
    "X_examp = np.hstack([temp_delta.T, np.arange(1881,2020).reshape(-1,1)])\n",
    "\n",
    "n_refits = 20\n",
    "name_linmodels = []\n",
    "name_linmodels_dic = {}\n",
    "test_scores = []\n",
    "\n",
    "for n in range(len(nametest_M_name)):\n",
    "    y_examp = temp_bdelta[n,:].T\n",
    "    \n",
    "    lin_mod = RidgeCV(cv=n_refits, alphas=[.1])\n",
    "    \n",
    "    #With this few data points, I'm throwing the whole thing in for now.\n",
    "    #The CV is using a subset each time already.\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_examp, y_examp, test_size=0.1)\n",
    "    X_train, y_train = X_examp, y_examp\n",
    "    lin_mod.fit(X_train, y_train)\n",
    "\n",
    "    name_linmodels.append(lin_mod)\n",
    "    \n",
    "    print('{0}: train {1:.3f} ({2})'.format(nametest_M_name.iloc[n],lin_mod.score(X_train, y_train), n))\n",
    "    y_pred_fu = lin_mod.predict(X_pred_arima)\n",
    "    if n%20==0:\n",
    "        plt.plot(X_train[:,-1], y_train, 'b.')\n",
    "        #plt.plot(X_test[:,-1], y_test, 'b+')\n",
    "        #plt.plot(X_test[:,-1], y_pred_st, 'rx')\n",
    "        #plt.plot(X_examp[-5:,-1], y_examp[-5:], 'c+')\n",
    "        #plt.plot(X_examp[-5:,-1], y_pred_fu[-5:], 'mx')\n",
    "        plt.plot(np.arange(2020,2025), y_pred_fu)\n",
    "        plt.legend(['Train','Test-actual','Test-pred','Val.-actual','Val.-pred'])\n",
    "        plt.show()\n",
    "    name_linmodels_dic[nametest_M_name.iloc[n]] = lin_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3: MAKE NAME PROJECTIONS\n",
    "\n",
    "n_names = len(nametest_M_name)\n",
    "\n",
    "name_futures = np.zeros([n_names,5])\n",
    "for n in range(n_names):\n",
    "    name_futures[n,:] = name_linmodels[n].predict(X_pred_arima)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testrun: First 30 names\n",
    "\n",
    "for n in range(30):\n",
    "    plt.plot(np.arange(1880,2020), nametest_M_birth[n,:])\n",
    "    plt.plot(np.arange(2020,2025), nametest_M_birth[n,-1] + name_futures[n,:] - name_futures[n,0], 'b+')\n",
    "    plt.title(nametest_M_name.iloc[n])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifications: I should do a log transform on the data early on, to convert it closer to a normal-distribution range of values, which would respond more appropriately to linear fitting. That's going near the top of the list, it will have a real effect on how well most models can characterize the data. \n",
    "\n",
    "Overly-optimistic predictions for near-dead names could be handled better at the individual-name level with ARIMA, despite the drastically slower processing. Similarly, ARIMA would likely also do fairly well avoiding negatives, since the graph tends to flatten as it nears zero. That's largely handled with the transition to yearly change, though. A hybrid model using both could probably do pretty well, but it would be difficult to reasonably fit given the data, and at that point you might want to head into more complex territory like RNN. \n",
    "\n",
    "Anyway, predicting names is inherently a difficult problem. We don't have reliable time components to the signal that we can account for, or a massive pile of datapoints. There are generation-related timing elements, but those are close to random, and how long trends persist may be changing. \n",
    "\n",
    "Pulling in data on name popularity on a state-by-state basis would help quite a bit, bumping our dataset from about 140 datapoints to about 5000 (some state reporting didn't get going until later if I'm recalling right, so it's not quite 140 x 51). It certainly wouldn't solve the problem by itself! State name trends are strongly correlated with national trends, especially as media increasingly moves away from the local level. State data could significantly help, but it will be necessary to use models that account for correlation between variables. \n",
    "\n",
    "Future directions for name trend prediction, in summary:\n",
    "- More in-depth data transformation prior to analysis/modeling (log or Box-Cox transform built into `pmdarima`, for example)\n",
    "- Consider multivariate autoregression (i.e. VAR) to model multivariate time series data more effectively (not likely to make the ARIMA fitting feasible for individual names without a days-long run)  \n",
    "- Consider reweighting recent years. Names with a strong recent increase are at a disadvantage in threshold, despite strong likelihood of continuing future presence. Could weigh name prevalence more highly in the last 3-5 years, for example, both to avoid cutting them off at the threshold stage and to increase the attention the model pays to recent trends? Something to consider.\n",
    "- Add state data to improve models (at minimum should reduce year-level noise, important now that it's predicting based on yearly change)\n",
    "- Toss this into an actual user interface/prediction engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assorted fun side comparisons:\n",
    "\n",
    "#Names in '50 ways to leave your lover' by Paul Simon:\n",
    "#https://www.azlyrics.com/lyrics/paulsimon/50waystoleaveyourlover.html\n",
    "#Jack, Stan, Roy, Gus, Lee\n",
    "\n",
    "#Names in 'Mambo No. 5' by Lou Bega:\n",
    "#https://www.azlyrics.com/lyrics/loubega/mambono5.html\n",
    "#Angela, Pamela, Sandra, Rita, Monica, Erica, Rita (again), Tina, Sandra, Mary, Jessica\n",
    "\n",
    "#Names in '88 lines about 44 wome' by The Nails:\n",
    "#https://genius.com/The-nails-88-lines-about-44-women-lyrics\n",
    "#Deborah, Carla, Mary, Susan, Reno, Cathy, Vicki, Kamela, Xylla, Joan, Sherry, Kathleen,\n",
    "#Seattle, Karen, Jeannie, Mary Ellen, Gloria, Mimi, Marilyn, Julie, Rhonda, Patty, Linda, \n",
    "#Katherine, Pauline, Jean-Marie, Gina, Jackie, Sarah, Janet, Tanya, Brenda, Rowena, Dee Dee, \n",
    "#Debbie Ray, Nina, Bobbi, Eloise, Terri, Ronnie, Jezebel, Dinah, Judy, Amaranta\n",
    "\n",
    "#State names:\n",
    "#(self-evident)\n",
    "\n",
    "#City names: \n",
    "#US cities by population (>100,000):\n",
    "#https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\n",
    "\n",
    "#Worldwide cities (>5,000,000):\n",
    "#https://en.wikipedia.org/wiki/List_of_largest_cities\n",
    "\n",
    "#Urban vs. rural states:\n",
    "#https://fivethirtyeight.com/features/how-urban-or-rural-is-your-state-and-what-does-that-mean-for-the-2020-election/\n",
    "#(Loosely, how many people are within 5 miles of a given census tract)\n",
    "\n",
    "#Check trendiness: It looks in general like the time course for F\n",
    "#names may be shorter than the time course for M names. Is that\n",
    "#true, and if so has it been becoming less true since 2000?\n",
    "\n",
    "#Could look at that by taking 10-year chunks (looks like about the\n",
    "#right length to capture a rising or falling trend) and seeing which\n",
    "#names correlate together for each chunk? Automatic grouping based\n",
    "#on \"trendiness\" would be pretty cool. \n",
    "\n",
    "#Extend data: I could also cross-correlate names with Google ngrams \n",
    "#results for that name, that would be a nice comparison. Excellent\n",
    "#proxy for whether a name is culturally established!\n",
    "\n",
    "#Extend data: Cross-correlate artists who made the Billboard top 10\n",
    "#using their names (i.e. Alanis). Only include cases where the name\n",
    "#is in both, so simple inner join would be ideal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.su"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

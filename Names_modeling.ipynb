{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41299a65",
   "metadata": {},
   "source": [
    "# Name clustering and prediction:\n",
    "\n",
    "Project goal: Predict what name trends will occur in a given year, based on the name properties I have calculated in the prior analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d48c78",
   "metadata": {},
   "source": [
    "## Preprocessing: Import libraries, generate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2bd5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import (most) necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import distance #Using the distance module for Levenshtein difference\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "pd.set_option('display.max_rows',150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed02db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "#Autocomplete has been finicky, switching versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d39168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import preprocessed data generated in \"Names_calculate_reduced\"\n",
    "#or prior actuarial calculations.\n",
    "\n",
    "#Data structures from prior actuarial, modified in Names_calculate_reduced:\n",
    "\n",
    "alive_F = pd.read_pickle('./processed_variables/life_F_df2.pkl')\n",
    "alive_M = pd.read_pickle('./processed_variables/life_M_df2.pkl')\n",
    "\n",
    "\n",
    "#Generated in 'Names_calculate_reduced':\n",
    "\n",
    "#Total births, from SSA:\n",
    "totalnames_table = pd.read_pickle('./processed_variables/Total_soc_cards.pkl')\n",
    "\n",
    "# #Names dataframe, HANDLED BELOW\n",
    "# names_df = pd.read_pickle('names_df.pkl')\n",
    "# names_df_trim = pd.read_pickle('names_df.pkl')\n",
    "\n",
    "#Year of birth data:\n",
    "namelife_M_yob = np.load('./processed_variables/namelife_M_yob.npy')\n",
    "namelife_F_yob = np.load('./processed_variables/namelife_F_yob.npy')\n",
    "#\"Base\" life data (total of all ages alive each year):\n",
    "namelife_M_base = np.load('./processed_variables/namelife_M_base.npy')\n",
    "namelife_F_base = np.load('./processed_variables/namelife_F_base.npy')\n",
    "#\"Name\" data: the names associated with each element:\n",
    "namelife_M_name = np.load('./processed_variables/namelife_M_name.npy', allow_pickle=True)\n",
    "namelife_F_name = np.load('./processed_variables/namelife_F_name.npy', allow_pickle=True)\n",
    "#\"Birth\" data: the base how-many-per-year for all names:\n",
    "namebirth_M = np.load('./processed_variables/namebirth_M.npy')\n",
    "namebirth_F = np.load('./processed_variables/namebirth_F.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ceb3b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2020863 entries, 0 to 32029\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   Name    object\n",
      " 1   Sex     object\n",
      " 2   Number  int64 \n",
      " 3   Year    int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 77.1+ MB\n",
      "Pretrim -> posttrim, minimum 20 births:\n",
      "\n",
      "Total names (total M/F separately):\n",
      "111472 -> 70589\n",
      "Total name/year records:\n",
      "2020863 -> 1956469\n",
      "\n",
      "Removes 0.37 of names while removing 0.032 of total birth records.\n"
     ]
    }
   ],
   "source": [
    "#Load the year-of-birth data, in \"yobYYYY.txt\" format csv files,\n",
    "#containing all names with over 5 births for the year \"YYYY\", \n",
    "#the four-digit year \n",
    "#\n",
    "#Data source: https://www.ssa.gov/OACT/babynames/limits.html, I\n",
    "#manually downloaded the national zip file that unpacks to a \n",
    "#folder containing the files below, renamed \"names_wYYYY\", where\n",
    "#\"YYYY\" is the four-digit year of the \n",
    "\n",
    "#REFRESH EVERY TIME. The \"names_df\" and \"names_df_trim\" data \n",
    "#structures take up just over 100MB, so they do not play nice \n",
    "#with Github. Takes maybe 10 seconds.\n",
    "\n",
    "refresh = True\n",
    "\n",
    "if refresh:\n",
    "\n",
    "    #Drop down into the correct directory,\n",
    "    os.chdir('../Names/names_w2020')\n",
    "    #grab a list of the files in it,\n",
    "    current_dir = os.listdir()\n",
    "\n",
    "    names_temp = []\n",
    "    #step through them, loading and appending\n",
    "    for ftemp in current_dir:\n",
    "        if ftemp[-3:] == 'txt':\n",
    "            dftemp = pd.read_csv(ftemp, names=['Name','Sex','Number'])\n",
    "            dftemp['Year'] = int(ftemp[3:-4])\n",
    "            names_temp.append(dftemp)\n",
    "\n",
    "    #chop them all together\n",
    "    names_df = pd.concat(names_temp)\n",
    "    names_df.info()\n",
    "\n",
    "    #go back up\n",
    "    os.chdir('..')\n",
    "\n",
    "\n",
    "    #Reset the indexes to unduplicate them, since each stack was indexed \n",
    "    #separately; keeping the old indices is handy though, so default\n",
    "    #'drop = false' option is kept.\n",
    "\n",
    "    #Yes, the sorting matters. It makes plotting way easier later on, and\n",
    "    #I could definitely use it to speed up the name structure indexing. \n",
    "\n",
    "    names_df = names_df.reset_index().sort_values(['Name','Year'])\n",
    "\n",
    "\n",
    "\n",
    "    #\"names_df\" is raw, not controlled for population or births. What was \n",
    "    #the total as a fraction of the year's name births? Currently only\n",
    "    #using the total names included in this data as the core comparison.\n",
    "    #Could pull from the social security total above in future changes.\n",
    "\n",
    "    year_total = names_df.groupby('Year').sum()\n",
    "\n",
    "    #Join year_total w/ names_df to get what fraction of named births\n",
    "    #that year made up of the given name/sex pair:\n",
    "    names_df = pd.merge(left=names_df, right=totalnames_table, \n",
    "                        left_on='Year', right_on='Year of birth', \n",
    "                        how='left')\n",
    "    names_df['Fraction'] = names_df['Number']/names_df['Total']\n",
    "\n",
    "    #Not normalizing to separate male or female births, only total.\n",
    "    #Possibly another future improvement, since the likelihood of \n",
    "    #registering males/females appears to have been biased early on. \n",
    "    names_df.drop(columns=['Male','Female','Total'], inplace=True)\n",
    "\n",
    "    #Calculate total number of births for each name:\n",
    "    totalbirths_byname = names_df[['Name','Sex','Number']].groupby(['Name','Sex']).sum()\n",
    "\n",
    "    trimvalue = 20\n",
    "    #Limit dataset to names that had more than 20 total births\n",
    "    #with that name:\n",
    "    keepnames = totalbirths_byname[totalbirths_byname['Number'] > trimvalue]\n",
    "\n",
    "    #Join on name/sex pairs to keep only ones included above\n",
    "    names_df_trim = pd.merge(left=names_df, right=keepnames, on=['Name','Sex'], how='inner')\n",
    "    #Handling number column ambiguity, not pretty but works fine\n",
    "    names_df_trim['Number'] = names_df_trim['Number_x']\n",
    "    names_df_trim.drop(columns=['Number_x', 'Number_y'], inplace=True)\n",
    "    val1 = len(names_df.groupby(['Name','Sex']).size())\n",
    "    val2 = len(names_df_trim.groupby(['Name','Sex']).size())\n",
    "    val1a = len(names_df)\n",
    "    val2a = len(names_df_trim)\n",
    "\n",
    "    #Display results\n",
    "    print('Pretrim -> posttrim, minimum {} births:\\n'.format(trimvalue))\n",
    "    print('Total names (total M/F separately):')\n",
    "    print(val1, '->', val2)\n",
    "    print('Total name/year records:')\n",
    "    print(val1a, '->', val2a)\n",
    "    print('\\nRemoves {:.2f} of names while removing {:.3f} of total birth records.'.format((val1-val2)/val1, (val1a-val2a)/val1a))\n",
    "\n",
    "    #names_df.to_pickle('names_df.pkl')\n",
    "    #names_df_trim.to_pickle('names_df_trim.pkl')\n",
    "    \n",
    "else:\n",
    "    names_df = pd.read_pickle('names_df.pkl')\n",
    "    names_df_trim = pd.read_pickle('names_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8cb84",
   "metadata": {},
   "source": [
    "## Step 1: Name Clustering\n",
    "\n",
    "**The goal:** Find names that are similar to one another in their properties as words in order to identify name trends. In the example in the prior analysis (`Names_visualize`), you may have noticed that both \"Darin\" and \"Darrin\" were characteristic names. This is common! The name \"Abigail\", for example, has been spelled at least a dozen different ways. If we treat all these names as independent, we may miss the broader-trend forest for the specific-spelling trees. **In this step, we'll cluster names based on how similarly they are spelled or pronounced.** \n",
    "\n",
    "Let's start with the spelling. I'll be using Levenshtein distance, a very well-characterized measure of how similar two words are to one another. It's also known as the \"edit distance\", since it measures the number of single-character insertions, deletions, or substitutions it takes to get from one word to another. Its simplicity is its strength, but it also means that Levenshtein distances doesn't take into account whether any given pair of letters are more similar (i.e. \"a\" is much more like \"e\" than \"q\"). \n",
    "\n",
    "The Levenshtein distance gives a relative difference between each word, which means we're looking for a method that isn't based on absolute position. **The core clustering method we'll use is affinity propagation**. For that, we will need to calculate a how similar every word is to every other word. \n",
    "\n",
    "Minor problem: Using the current set of names, the number of comparisons needed to calculate a complete distance matrix is big. The **total number of comparisons is O(nÂ²)**, which is not great when you have a set of 100,000 names. This is one key reason for pruning the dataset. Second, limiting the dataset also improves the likelihood that our clustering method converges. From practical testing, it appears that when given any single Levenshtein distance measure, the dataset is unlikely to converge if it contains more than a few thousand items (see next section for more detail). To trim the dataset, I'll be focusing on the most relevant names, which met a certain threshold of use. This trimming occurs on top of an earlier dataset reduction, which only kept names with at least 20 nameholders born. \n",
    "\n",
    "In summary, to make our clustering faster and more functional, **we'll limit our dataset again, based on peak holders alive.** In the plots below, I show how some additional thresholds affect the number of names. \n",
    "\n",
    "This does not prevent us from analyzing additional names after the fact! For a new name, we can easily calculate which existing cluster it should be part of (for example, lowest average Levenshtein distance from all members of that cluster), but the automatic detection of clusters itself does not work as nicely. Given the matrix size and the limitations of our distance estimate, this may be close to what we can practically get away with using a *single* Levenshtein distance and affinity propagation. \n",
    "\n",
    "### Code subsections:\n",
    "**- Trimming the dataset**  \n",
    "**- Gathering pronunciations**  \n",
    "**- Calculating name distances**  \n",
    "**- Clustering**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520d621b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: set is previously trimmed to only names with at least 20 born,\n",
      "which is why there's a sharp shoulder at 20 alive\n",
      "Total male names >50: 20482\n",
      "Total female names >50: 33056\n",
      "Total male names >6000: 1371\n",
      "Total female names >6000: 2022\n"
     ]
    }
   ],
   "source": [
    "#TRIMMING THE DATASET:\n",
    "\n",
    "#Compare how many names you keep, depending on your cutoff threshold\n",
    "#for minimum number of holders (requires hand-tuning, takes a couple\n",
    "#seconds to run)\n",
    "\n",
    "n_years = namebirth_M.shape[1]\n",
    "\n",
    "commons_list_M = []\n",
    "commons_list_F = []\n",
    "#Test thresholds of 1-100000, using 20 log steps\n",
    "testvals = np.logspace(0,5,20)\n",
    "for n in testvals:\n",
    "    commons_list_M.append(sum(namelife_M_base.max(1) > n))\n",
    "    commons_list_F.append(sum(namelife_F_base.max(1) > n))\n",
    "    \n",
    "plt.loglog(testvals, commons_list_M, 'b', testvals, commons_list_F, 'r')\n",
    "plt.xlabel('Minimum people of a given name alive at one time for inclusion')\n",
    "plt.ylabel('Number of total names')\n",
    "plt.legend(['Male','Female'])\n",
    "plt.show()\n",
    "plt.plot(testvals, commons_list_M, 'b', testvals, commons_list_F, 'r')\n",
    "plt.xlabel('Minimum people of a given name alive at one time for inclusion')\n",
    "plt.ylabel('Number of total names')\n",
    "plt.legend(['Male','Female'])\n",
    "plt.ylim([0,10000])\n",
    "plt.show()\n",
    "\n",
    "#A couple specific test values:\n",
    "thresh1 = 50\n",
    "thresh2 = 6000\n",
    "commons_M_1 = namelife_M_base.max(1) > thresh1\n",
    "commons_F_1 = namelife_F_base.max(1) > thresh1\n",
    "commons_M_2 = namelife_M_base.max(1) > thresh2\n",
    "commons_F_2 = namelife_F_base.max(1) > thresh2\n",
    "\n",
    "#Total names:\n",
    "print('Note: set is previously trimmed to only names with at least 20 born,\\nwhich is why there\\'s a sharp shoulder at 20 alive')\n",
    "print('Total male names >{}: '.format(thresh1) + str(sum(commons_M_1)))\n",
    "print('Total female names >{}: '.format(thresh1) + str(sum(commons_F_1)))\n",
    "print('Total male names >{}: '.format(thresh2) + str(sum(commons_M_2)))\n",
    "print('Total female names >{}: '.format(thresh2) + str(sum(commons_F_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9f626",
   "metadata": {},
   "source": [
    "### The problem of distance estimation, in detail:\n",
    "\n",
    "What is it about our distance estimation that may cause problems? Levenshtein distances are very quantized. All the distances between words are integers. There are a lot of names with a distance of 1, 2, or 3, for example, which means there aren't any smooth grades of difference that let the algorithm find a good threshold to separate them. Consider, for example, whether \"Chris\" is closer to \"Cris\" or to \"Caris\". We'd probably say the former, but the spelling distance (1) is identical. Even with normalized Levenshtein, which measures the fraction of the word that is changed, there are many similar distances between different name pairs (like the prior example, for one direction), which is likely to cause problems for any clustering method. \n",
    "\n",
    "There are several ways we could approach this problem. One it to simply find more dimensions along which we can compare distance! If a comparison name is the same distance away in spelling, it may be closer or farther in pronunciation, which gives us the potential for much smoother grades of difference. (Figuring out pronunciation for a name is difficult, but I found a reasonable way to offboard it. Let's continue, assuming we have pronunciation information.)\n",
    "\n",
    "First, we could use the Levenshtein distances across IPA pronunciation of each name, rather than spelling. The IPA pronunciation for Chris, for example, is \"krËÉªs\", with one character for each included sound. Looking at the number of insertions/deletions/substitutions of sounds between names is a good way to measure how similar names are, and is at least partially independent from their character differences. The IPA pronunciation works great when compared using a basic Levenshtein distance, since each character is a sound, and additions and subtractions make sense. However, it still runs into the same problem we had with spelling: there's no naive way to tell *how* similar individual characters are, despite, for example, \"bee\" being closer to \"bye\" than to \"br\".\n",
    "\n",
    "Second, then, we could implement a similarity metric using ARPABET pronunciation. The ARPABET pronunciation for Chris is \"K R IH1 S\". You may notice that that vowel has a lot more characters. The ARPABET model contains useful information about sound similarity, just from the overlap between the characters present in each sound's code. We could use this directly: The prior comparison of Chris, \"K R IH1 S\", vs. Caris, \"K AE1 R IH0 S\", however, shows a minor downside of that. Individual characters do not represent individual sounds. The ARPABET Levenshtein distance  may over or underestimate how far apart words with different sounds are, depending on the number of characters used to represent the sound. \n",
    "\n",
    "So... what do we do with ARPABET? First, it's important to note that the problem I mentioned above isn't prohibitive! Basic Levenshtein on ARPABET still gives us a different perspective about name similarity, even if it's what you might call a noisy signal. \n",
    "\n",
    "In short, we have a problem and an opportunity: Pronunciation data has some of the same quantization challenges as spelling data, but we also know that some sounds are more like one another, which our current edit distance misses. Here, before I just use the simple solution and call it a day, I'll talk about how I could implement more precise, high-effort solutions.\n",
    "\n",
    "### High-effort solutions:\n",
    "\n",
    "#### ARPABET multi-level distance measures:\n",
    "If ARPABET represents individual sounds with varying sets of characters, perhaps we could measure distance on multiple levels: calculate character distances between individual sounds, then use those distances to calculate how far apart aggregate collections of sounds are. \n",
    "\n",
    "One relatively simple way would be doing normalized character difference rather than total character substitution. For example, the \"A\" in \"Adrian\" is represented by \"EY1\", compared to the \"A\" in \"Andrew\" which is \"AE1\". Using standard Levenshtein edit distance between these two ARPABET representations of the characters, we could for example make one deletion (EY1 -> E1) and one insertion (E1 -> AE1). To go from EY1 to AE1 is be a Levenshtein distance of 2, or a normalized Levenshtein distance of 0.66 (fraction of characters changed).\n",
    "\n",
    "That's not great, since it gives a distance of 0.66 between the two characters despite pretty similar sounds, but it's still more effective than the full 1.00 distance that IPA pronunciation without custom character differences would give. We could also ignore the order of the letters, and do a simple character-similarity measure, which would give us a 0.33 distance, but I'd have to look into whether this would give spurious correlations between unrelated sounds. Once you're at the level of sounds, sound insertion/deletion would still be a binary scale, but substitutions could be more nuanced. Unfortunately, this solution would also require a modified Levenshtein-like implementation to use this multi-level system, (or similarly in-depth investigation of alternative solutions to see which could be adapted for this purpose).\n",
    "\n",
    "#### IPA weighted distance using sound similarity:\n",
    "Instead of developing a new multi-level version of the Levenshtein distance, it would be easier to characterize how similar the IPA sounds are, and use a single-level Levenshtein with weighted substitutions. However, finding quantitative differences between IPA characters is not trivial. I was able to find a paper that looks like it does a good job measuring how far apart different vowels are using both text-similarity and pronunciation-similarity methods, [by Wieling et al (2012)](http://www.martijnwieling.nl/files/WielingMargarethaNerbonne2012.pdf). Unfortunately, while the methods are very interesting and the results look good, the paper does not provide a straighforward distance matrix. The sound distances could be partially reconstructed from Figure 3, but, again, starting to get a bit past the scope of the project.\n",
    "\n",
    "#### Both rely on weighted Levenshtein at the top level:\n",
    "In either case, the differences between sounds vary, with some being closer and some farther, as mentioned above. At the level of the name as a series of sounds, this calls for a weighted Levenshtein, where different distances are no longer all 1. Fortunately, this problem is fairly well-studied, and there appear to be [multiple toolboxes that implement it](https://stackoverflow.com/questions/6080958/ocr-weighted-levenshtein-distance), such as the [\"weighted-levenshtein\" package](https://pypi.org/project/weighted-levenshtein/). \n",
    "\n",
    "### High effort options are out there, but why not go simple?\n",
    "I've laid out two potential solutions for calculating pronunciation similarity, but implementing these is beyond the scope of this project. Instead, I use a very \"data science\"-type solution: **Solve the problem in a passable way using several different easy methods, then build a model that uses them all.** Here, I calculate name distances based on three measures: spelling edit distance, IPA edit distance, and ARPABET edit distance. I slightly decreased the weight of the pronunciation measures relative to spelling, since both pronunciation options should be measuring the same thing. \n",
    "\n",
    "While each individual measure is quantized, combining this set of three measures gets us a very wide range of potential distances. Compared to the roughly 1-12 options from the raw spelling edit distance, there are many more now possible, and we have a great deal more uniqueness in our word distances. \n",
    "\n",
    "*POTENTIAL FUTURE CHANGE:* \n",
    "*Pre-group names with identical predicted pronunciations, then get the top set. That will, for example, catch the \"Abigail\" complex under a single umbrella, or the \"Brayden/Breighdon\" etc. name clan.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0951c6",
   "metadata": {},
   "source": [
    "\n",
    "### Gathering pronunciations:\n",
    "\n",
    "To do anything with pronunciation distance, we have to actually get the pronunciations. Rather than trying to develop or implement my own text-to-speech converter for arbitrary collections of English letters, I used an existing API to get estimated pronunciations for each name. Datamuse is one of the oldest Internet dictionaries, which means it's accumulated some interested features over the years. The API has very in-depth options, as described in [the extensive documentation](https://www.datamuse.com/api/). Unlike some sources, it gives estimates for arbitrary words, not just those included in established sources like the [Carnegie Mellon Pronouncing Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict). It does fairly well, though some names get lumped in with related names as the same pronunciation. Abrielle, for example, comes out as identical to Gabrielle, because it was close enough that the tool guessed it has a leading G. These occasional questionable pronunciations are a clear disadvantage of this method, but it's a very useful tool considering. \n",
    "\n",
    "*Personally, I often rely on hearing others to know a name's correct pronunciation, rather than determining it from text. English pronunciation is hard enough already, since you have to have heard a word to be reliably accurate, even knowing which contributing language a word came from. Names are no easier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35897e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GATHERING PRONUNCIATIONS:\n",
    "\n",
    "#Example API: https://api.datamuse.com/words?sl=mackenzie&md=r\n",
    "#This API call returns both a word's pronunciation and words that\n",
    "#sound like it.\n",
    "\n",
    "import string\n",
    "from ediblepickle import checkpoint\n",
    "\n",
    "#Getting all pronunciations takes a lot of API calls. To cut down\n",
    "#on the time, this function is checkpointed, and the function \n",
    "#responses are stored in a \"name_pronunciation_storage\" folder.\n",
    "\n",
    "@checkpoint(key=string.Template('name_pronunc_{0}_{1}.pkl'), work_dir='name_pronunciation_storage', refresh=False)\n",
    "def get_name_pronunc(name, sex):\n",
    "    #Define core API call, which works great with the word itself at the end\n",
    "    baseapi = 'https://api.datamuse.com/words?md=r&ipa=1&sl='\n",
    "    #Get page response for api+name.\n",
    "    \n",
    "    #V1: Use futuressessions\n",
    "    #REQUIRES THAT A FUTURESSESSION BE ACTIVE! \"session = FuturesSession()\"\n",
    "    resp = session.get(baseapi + name)\n",
    "    #Pull out just the content\n",
    "    pagecont = resp.result().content\n",
    "    pagetext = resp.result().text\n",
    "    \n",
    "    #V2: Use a standard request\n",
    "    #resp = requests.get(baseapi + name)\n",
    "    #pagecont = resp.content\n",
    "    #pagetext = resp.text\n",
    "    \n",
    "    #Fairly messy way to do this, probably ought to use a class\n",
    "    sylstart = pagetext.find('numSyllables')\n",
    "    syl = int(pagetext[sylstart+14])\n",
    "    arpastart = pagetext.find('pron:')\n",
    "    arpastop = pagetext.find('\"', arpastart)\n",
    "    arpa = pagetext[arpastart+5:arpastop]\n",
    "    ipastart = pagetext.find('ipa_pron:')\n",
    "    ipastop = pagetext.find('\"', ipastart)\n",
    "    ipa = pagetext[ipastart+9:ipastop]\n",
    "    \n",
    "    #print(ipa)\n",
    "    pronunc = []\n",
    "    pronunc.append(syl)\n",
    "    pronunc.append(arpa)\n",
    "    pronunc.append(ipa)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "    return pronunc, pagetext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a868142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.9 ms, sys: 4.1 ms, total: 65 ms\n",
      "Wall time: 65.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "redo_pronunc = False\n",
    "#Should functionize this. \n",
    "#Note that this script runs names that appear in both male and\n",
    "#female name lists twice. Because the load process is very \n",
    "#quick, I have not optimized it out.\n",
    "#\n",
    "#Script run times and storage:\n",
    "#1. If the data is loaded from the all_pronunciations pickle\n",
    "#   files, the script takes less than a second. \n",
    "#2. If the data is reloaded but restored from checkpoints,\n",
    "#   the script takes about a minute. \n",
    "#3. If the data is not checkpointed yet, the base collection\n",
    "#   takes multiple hours, and writes a folder containing 110k\n",
    "#   items, which take up about 1GB of memory. The data storage\n",
    "#   is overkill: it includes information about similar words,\n",
    "#   but we are currently only using the pronunciation data for\n",
    "#   the target word. If it saves rerunning this fresh, though,\n",
    "#   that extra memory is WELL worth it. \n",
    "\n",
    "if redo_pronunc:\n",
    "    session = FuturesSession()\n",
    "    all_pronunciations_F = {}\n",
    "    all_pagecont_F = []\n",
    "\n",
    "    for name in namelife_F_name:\n",
    "        pronunc, pagecont = get_name_pronunc(name, 'F')\n",
    "        all_pronunciations_F[name] = pronunc\n",
    "        all_pagecont_F.append(pagecont)\n",
    "        \n",
    "    with open('./processed_variables/all_pronunciations_F.pkl', 'wb') as fname:\n",
    "        pickle.dump(all_pronunciations_F, fname)\n",
    "        \n",
    "else:\n",
    "    with open('./processed_variables/all_pronunciations_F.pkl', 'rb') as fname:\n",
    "        all_pronunciations_F = pickle.load(fname)\n",
    "\n",
    "if redo_pronunc:\n",
    "    session = FuturesSession()\n",
    "    all_pronunciations_M = {}\n",
    "    all_pagecont_M = []\n",
    "\n",
    "    for name in namelife_M_name:\n",
    "        pronunc, pagecont = get_name_pronunc(name, 'M')\n",
    "        all_pronunciations_M[name] = pronunc\n",
    "        all_pagecont_M.append(pagecont)\n",
    "        \n",
    "    with open('./processed_variables/all_pronunciations_M.pkl', 'wb') as fname:\n",
    "        pickle.dump(all_pronunciations_M, fname)\n",
    "        \n",
    "else:\n",
    "    with open('./processed_variables/all_pronunciations_M.pkl', 'rb') as fname:\n",
    "        all_pronunciations_M = pickle.load(fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0da56",
   "metadata": {},
   "source": [
    "#### General name fitting notes:\n",
    "\n",
    "Time course-based analysis like this could be done using a RNN (Recurrent Neural Network) via the Tensor Flow library, it's well-implemented in Keras. This is a good future modeling opportunity. \n",
    "\n",
    "\n",
    "#### Possible predictive analyses:\n",
    "\n",
    "1. Predict with a linear model how popular a given name is expected to be in a given year, based on popularity of other names and the name's own popularity in the past. What are some potential downfalls? There are a large number of names, and 140 data points. Overfitting could be a problem. It may be a good idea to predict using aggregated trends, rather than individual name trends, by clustering names based on spelling/pronunciation similarity, then predicting based on trends for clusters rather than individual names. \n",
    "\n",
    "2. Predict name frequency in a given year based on number and ages (or age structure) of each other name currently alive. This would add many more variables to an already variable-rich fit, but could be very interesting: generational name trends would be easy to account for by this method. However, a lot of this data could largely be accounted for with the panel regression, as simple negative relationships between name frequencies would explain it.\n",
    "\n",
    "3. Predict name trends using Panel Linear Regression. This is probably the most appropriate, given the data types and properties that we have. I'll be relying in part on a [writeup of Panel Data Regression in Python](https://towardsdatascience.com/a-guide-to-panel-data-regression-theoretics-and-implementation-with-python-4c84c5055cf8) that covers a lot of the potential pitfalls. It may also be worth considering a [missing-data implementation](https://www.researchgate.net/publication/349523278_Bayesian_estimation_and_model_comparison_for_linear_dynamic_panel_models_with_missing_values), but that version is probably not necessary: we have nearly-complete data for our subset of names here, if we assume that years with less than 5 births of any given name are effectively zero. \n",
    "\n",
    "\n",
    "#### Other considerations:\n",
    "\n",
    "- Clustering methods: Currently uses spelling/pronunciation, as mentioned above, but we could also use the time course correlation between different names over time. Just calculate the correlation matrix between all the name-births time series. This would allow name grouping based on trends, not spelling/pronunciation, and could be included as a third measure for distance, or as a separate clustering method. It might also be good to look at \"typical\" correlations, over say a 20-30 year span\n",
    "\n",
    "- Alternative clustering methods: Spectral Clustering, Markov Clustering (MCL), other distance-based rather than position-based clustering. We really don't have an absolute position estimate at all here, it's all relative. \n",
    "\n",
    "- More detailed name spelling/pronunciation comparison: Customized or weighted Levenshtein would be a possible option, as described in earlier notes. NLP could also be useful, with (for example) bag-of-words comparison for individual letters and letter bigrams, rather than word/word bigrams. Keeping capitals would also be very helpful. \n",
    "\n",
    "- Partially-manual clustering: Names with *identical* pronunciation are automatically clustered, then the remaining names and name clusters are used as the base for affinity propagation clustering. \n",
    "\n",
    "- Clustering time-series!\n",
    "https://www.researchgate.net/publication/4756297_Model-Based_Clustering_of_Multiple_Time_Series.\n",
    "Looks like it would be some work to implement, but potentially really interesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b33d78e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example name comparison, spelling vs. sounds:\n",
      "\n",
      "Andrew\n",
      "[2, 'AE1 N D R UW0 ', 'ËÃ¦ndru']\n",
      "Adrian\n",
      "[3, 'EY1 D R IY0 AH0 N ', 'ËeÉªdriÊn']\n",
      "\n",
      "Distances:\n",
      "Spelling Lev:\n",
      "0.6666666666666666\n",
      "IPA Lev:\n",
      "0.625\n",
      "ARPABET Lev:\n",
      "0.6111111111111112\n",
      "(ARPABET first sound Lev: AE1 vs EY1)\n",
      "( 0.6666666666666666 )\n"
     ]
    }
   ],
   "source": [
    "#Example name comparison\n",
    "\n",
    "testname1 = 'Andrew'\n",
    "testname2 = 'Adrian'\n",
    "\n",
    "print('Example name comparison, spelling vs. sounds:\\n')\n",
    "print(testname1)\n",
    "print(all_pronunciations_M[testname1])\n",
    "print(testname2)\n",
    "print(all_pronunciations_M[testname2])\n",
    "print('\\nDistances:\\nSpelling Lev:')\n",
    "print(distance.nlevenshtein(testname1,testname2))\n",
    "print('IPA Lev:')\n",
    "print(distance.nlevenshtein(all_pronunciations_M[testname1][2], all_pronunciations_M[testname2][2]))\n",
    "print('ARPABET Lev:')\n",
    "print(distance.nlevenshtein(all_pronunciations_M[testname1][1], all_pronunciations_M[testname2][1]))\n",
    "\n",
    "sound1 = all_pronunciations_M[testname1][1].split()[0]\n",
    "sound2 = all_pronunciations_M[testname2][1].split()[0]\n",
    "print('(ARPABET first sound Lev: ' + sound1 + ' vs ' + sound2 + ')')\n",
    "print('(', distance.nlevenshtein(sound1, sound2), ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72516fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names in set:\n",
      " M:  1371 \n",
      " F:  2022 \n",
      " S:  3393 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Define the nametest sets, using threshold from above:\n",
    "\n",
    "#Trimmed from >5000 to >6000, which keeps the all-names \n",
    "#distance structures under 100MB. \n",
    "thresh=6000\n",
    "\n",
    "nametest_M_inds = namelife_M_base.max(1) > thresh\n",
    "\n",
    "nametest_M_name = namelife_M_name[nametest_M_inds]\n",
    "nametest_M_pronI = pd.Series([all_pronunciations_M[tempname][2] for tempname in nametest_M_name])\n",
    "nametest_M_pronA = pd.Series([all_pronunciations_M[tempname][1] for tempname in nametest_M_name])\n",
    "nametest_M_num = namelife_M_base[nametest_M_inds].max(1)\n",
    "nametest_M_birth = namebirth_M[nametest_M_inds,:]\n",
    "\n",
    "nametest_F_inds = namelife_F_base.max(1) > thresh\n",
    "nametest_F_name = namelife_F_name[nametest_F_inds]\n",
    "nametest_F_pronI = pd.Series([all_pronunciations_F[tempname][2] for tempname in nametest_F_name])\n",
    "nametest_F_pronA = pd.Series([all_pronunciations_F[tempname][1] for tempname in nametest_F_name])\n",
    "nametest_F_num = namelife_F_base[nametest_F_inds].max(1)\n",
    "nametest_F_birth = namebirth_F[nametest_F_inds,:]\n",
    "\n",
    "#All names test set:\n",
    "nametest_S_name = np.concatenate([nametest_M_name, nametest_F_name])\n",
    "nametest_S_pronI = np.concatenate([nametest_M_pronI, nametest_F_pronI])\n",
    "nametest_S_pronA = np.concatenate([nametest_M_pronA, nametest_F_pronA])\n",
    "nametest_S_num = np.concatenate([nametest_M_num, nametest_F_num])\n",
    "nametest_S_birth = np.concatenate([nametest_M_birth, nametest_F_birth])\n",
    "\n",
    "print('Names in set:\\n',\n",
    "      'M: ', nametest_M_name.shape[0], '\\n',\n",
    "      'F: ', nametest_F_name.shape[0], '\\n',\n",
    "      'S: ', nametest_S_name.shape[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4acb5cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.6 ms, sys: 143 ms, total: 189 ms\n",
      "Wall time: 295 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#CALCULATING NAME DISTANCES\n",
    "\n",
    "#Despite the long preamble, the final distance math\n",
    "#I settled on is straightforward. You calculate the \n",
    "#normalized Levenshtein distance between each pair of\n",
    "#names. Simple to code, but expensive to run, so I'm\n",
    "#serializing the variables afterwards for loading.\n",
    "#Using a threshold of >5000 nameholders alive, which\n",
    "#limits the data to 1500M/2300F, the distance math\n",
    "#takes a couple hours if you do M/F/S, where \"S\" has\n",
    "#both.\n",
    "\n",
    "#I did look to see if there was a decent matrix math\n",
    "#option that would speed it up, but wasn't able to find\n",
    "#anything for this library. \n",
    "\n",
    "#Note: Calculating the ARPABET distance (pronA) takes\n",
    "#noticeably more time than spelling (name) or IPA (pron),\n",
    "#since the strings are a lot longer. Might be worth \n",
    "#taking out the spaces?\n",
    "\n",
    "\n",
    "#LIST COMPREHENSION VERSION EATS TOO MUCH MEMORY\n",
    "# lev_similarity_name_M = 1-np.array([[distance.nlevenshtein(nameM1,nameM2) for nameM1 in nametest_M_name] for nameM2 in nametest_M_name])\n",
    "\n",
    "#This ought to be done with a class, but copy-paste\n",
    "#works quickly, and we'll be running a limited set.\n",
    "\n",
    "n_names_M = nametest_M_name.shape[0]\n",
    "n_names_F = nametest_F_name.shape[0]\n",
    "n_names_S = nametest_S_name.shape[0]\n",
    "\n",
    "#Build arrays to write to\n",
    "levdif_name_M = np.zeros([n_names_M, n_names_M])\n",
    "levdif_pronI_M = np.zeros([n_names_M, n_names_M])\n",
    "levdif_pronA_M = np.zeros([n_names_M, n_names_M])\n",
    "levdif_name_F = np.zeros([n_names_F, n_names_F])\n",
    "levdif_pronI_F = np.zeros([n_names_F, n_names_F])\n",
    "levdif_pronA_F = np.zeros([n_names_F, n_names_F])\n",
    "levdif_name_S = np.zeros([n_names_S, n_names_S])\n",
    "levdif_pronI_S = np.zeros([n_names_S, n_names_S])\n",
    "levdif_pronA_S = np.zeros([n_names_S, n_names_S])\n",
    "\n",
    "refresh = False\n",
    "\n",
    "if refresh:\n",
    "    for i in range(n_names_M):\n",
    "        for j in range(n_names_M):\n",
    "            levdif_name_M[i,j] = distance.nlevenshtein(nametest_M_name[i], nametest_M_name[j])\n",
    "            levdif_pronI_M[i,j] = distance.nlevenshtein(nametest_M_pronI[i], nametest_M_pronI[j])\n",
    "            levdif_pronA_M[i,j] = distance.nlevenshtein(nametest_M_pronA[i], nametest_M_pronA[j])\n",
    "    print('M done')\n",
    "    \n",
    "    for i in range(n_names_F):\n",
    "        for j in range(n_names_F):\n",
    "            levdif_name_F[i,j] = distance.nlevenshtein(nametest_F_name[i], nametest_F_name[j])\n",
    "            levdif_pronI_F[i,j] = distance.nlevenshtein(nametest_F_pronI[i], nametest_F_pronI[j])\n",
    "            levdif_pronA_F[i,j] = distance.nlevenshtein(nametest_F_pronA[i], nametest_F_pronA[j])\n",
    "    print('F done') \n",
    "    \n",
    "    for i in range(n_names_S):\n",
    "        for j in range(n_names_S):\n",
    "            levdif_name_S[i,j] = distance.nlevenshtein(nametest_S_name[i], nametest_S_name[j])\n",
    "            levdif_pronI_S[i,j] = distance.nlevenshtein(nametest_S_pronI[i], nametest_S_pronI[j])\n",
    "            levdif_pronA_S[i,j] = distance.nlevenshtein(nametest_S_pronA[i], nametest_S_pronA[j])\n",
    "    print('S done')\n",
    "    \n",
    "    #Store generated variables\n",
    "    #I know it's good practice to with-open, but man does it take up space.\n",
    "    with open('./processed_variables/levdif_name_M_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_name_M, fname)\n",
    "    with open('./processed_variables/levdif_pronI_M_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_pronI_M, fname)\n",
    "    with open('./processed_variables/levdif_pronA_M_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_pronA_M, fname)\n",
    "        \n",
    "    with open('./processed_variables/levdif_name_F_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_name_F, fname)\n",
    "    with open('./processed_variables/levdif_pronI_F_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_pronI_F, fname)\n",
    "    with open('./processed_variables/levdif_pronA_F_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_pronA_F, fname)\n",
    "        \n",
    "    with open('./processed_variables/levdif_name_S_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_name_S, fname)\n",
    "    with open('./processed_variables/levdif_pronI_S_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_pronI_S, fname)\n",
    "    with open('./processed_variables/levdif_pronA_S_{}.pkl'.format(thresh), 'wb') as fname:\n",
    "        pickle.dump(levdif_pronA_S, fname)\n",
    "\n",
    "else:\n",
    "    #Load generated variables\n",
    "    with open('./processed_variables/levdif_name_M_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_name_M = pickle.load(fname)\n",
    "    with open('./processed_variables/levdif_pronI_M_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_pronI_M = pickle.load(fname)\n",
    "    with open('./processed_variables/levdif_pronA_M_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_pronA_M = pickle.load(fname)\n",
    "\n",
    "    with open('./processed_variables/levdif_name_F_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_name_F = pickle.load(fname)\n",
    "    with open('./processed_variables/levdif_pronI_F_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_pronI_F = pickle.load(fname)\n",
    "    with open('./processed_variables/levdif_pronA_F_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_pronA_F = pickle.load(fname)\n",
    "\n",
    "    with open('./processed_variables/levdif_name_S_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_name_S = pickle.load(fname)\n",
    "    with open('./processed_variables/levdif_pronI_S_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_pronI_S = pickle.load(fname)\n",
    "    with open('./processed_variables/levdif_pronA_S_{}.pkl'.format(thresh), 'rb') as fname:\n",
    "        levdif_pronA_S = pickle.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c0ee5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit 1: Spelling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chmullens/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_affinity_propagation.py:247: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "  \"will not have any cluster centers.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters:  1\n",
      "Fit 2: IPA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chmullens/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_affinity_propagation.py:247: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "  \"will not have any cluster centers.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters:  1\n",
      "Fit 3: ARPABET\n",
      "Clusters:  185\n",
      "Fit 3: Triple-distance, sum\n",
      "Clusters:  261\n",
      "Fit 4: Triple-distance, vector\n",
      "Clusters:  270\n",
      "CPU times: user 7.42 s, sys: 15.3 ms, total: 7.43 s\n",
      "Wall time: 7.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#CLUSTERING\n",
    "\n",
    "#Pretty quick! Once the similarity matrices are built, the cluster fitting\n",
    "#only takes a few seconds.\n",
    "\n",
    "#Pull in the affinity propagation module\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "#Set up an affinity propagation instance. Here, I used the\n",
    "#total population of each name as the preference, so that\n",
    "#more popular names would be more likely to be used as a\n",
    "#cluster center:\n",
    "pref_M = np.sum(nametest_M_birth, axis=1)\n",
    "pref_M = (pref_M/pref_M.max())**0.5 #Scaling to increase separation\n",
    "\n",
    "def preproc_distances(distances):\n",
    "    #Scaling to increase poor-fit name separation:\n",
    "    affinity = (1 - (distances/distances.max()))**2\n",
    "    return affinity\n",
    "\n",
    "#Initialize:\n",
    "affprop_M_name = AffinityPropagation(affinity='precomputed', preference=pref_M, random_state=10)\n",
    "affprop_M_pronI = AffinityPropagation(affinity='precomputed', preference=pref_M, random_state=10)\n",
    "affprop_M_pronA = AffinityPropagation(affinity='precomputed', preference=pref_M, random_state=10) #see note by .fit\n",
    "\n",
    "#The fit needs affinity, not difference, so 1 - difference (which ranges 0-1)\n",
    "\n",
    "print('Fit 1: Spelling')\n",
    "affprop_M_name.fit(preproc_distances(levdif_name_M))\n",
    "print('Clusters: ', len(np.unique(affprop_M_name.labels_)))\n",
    "print('Fit 2: IPA')\n",
    "affprop_M_pronI.fit(preproc_distances(levdif_pronI_M))\n",
    "print('Clusters: ', len(np.unique(affprop_M_pronI.labels_)))\n",
    "print('Fit 3: ARPABET')\n",
    "affprop_M_pronA.fit(preproc_distances(levdif_pronA_M))\n",
    "print('Clusters: ', len(np.unique(affprop_M_pronA.labels_)))\n",
    "#The ARPABET pronunciation does not usually converge if it's\n",
    "#the only distance measure. However, it's still useful to add\n",
    "#as another view on pronunciation. In practice, the clusters\n",
    "#seem to be more intuitively sensible when it is included.\n",
    "\n",
    "#After testing, the most sensible clusters happen when \n",
    "#weighting down the pronunciation distances to about 0.75 \n",
    "#each, so that they do not make up twice as much of the \n",
    "#total distance as the spelling does.\n",
    "\n",
    "#Simple version: Just add the distances of the three\n",
    "print('Fit 4: Triple-distance, sum')\n",
    "affprop_M_triple = AffinityPropagation(affinity='precomputed', preference=pref_M, random_state=10)\n",
    "aff_M = ( preproc_distances(levdif_name_M + 0.75*levdif_pronI_M + 0.75*levdif_pronA_M) )\n",
    "affprop_M_triple.fit(aff_M)\n",
    "print('Clusters: ',len(np.unique(affprop_M_triple.labels_)))\n",
    "\n",
    "#Alternate version: Calculate vector distance, assuming all are perpendicular\n",
    "print('Fit 5: Triple-distance, vector')\n",
    "affprop_M_triple2 = AffinityPropagation(affinity='precomputed', preference=pref_M, random_state=10)\n",
    "aff_M2 = preproc_distances(np.sqrt(    np.square(levdif_name_M) + \\\n",
    "                                   0.5*np.square(levdif_pronI_M) + \\\n",
    "                                   0.5*np.square(levdif_pronA_M)))\n",
    "affprop_M_triple2.fit(aff_M2) #Scaling to increase separation\n",
    "print('Clusters: ',len(np.unique(affprop_M_triple2.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff097e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLUSTERING FUNCTIONS\n",
    "\n",
    "#Pretty quick! Once the similarity matrices are built, the cluster fitting\n",
    "#only takes a few seconds.\n",
    "\n",
    "#Pull in the affinity propagation module\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "#Scaling: Goal here is to scale data such that more distant names are \n",
    "#somewhat further apart; i.e. increase the difference between 0.66 and \n",
    "#0.33, so that 0.66 registers as closer and 0.33 registers as farther.\n",
    "#Looks like exponential functions are the easiest to scale to encourage\n",
    "#finding a larger number of separate clusters. Input distances are 0 to\n",
    "#1, so all values will fall between 1 and 10. \n",
    "def defaultscaling(x):\n",
    "    return 10 ** x\n",
    "\n",
    "def preproc_distances(distances, scalingfunc=defaultscaling):\n",
    "    #Scaling to increase poor-fit name separation:\n",
    "    affinity = scalingfunc(1 - (distances/distances.max()))\n",
    "    return affinity\n",
    "\n",
    "def get_clusters(dists, births, scalingfunc=defaultscaling, randstate=10):\n",
    "    pref = np.sum(births, axis=1)\n",
    "    pref = (pref/pref.max())**1 #Weights can also be scaled to increase separation\n",
    "\n",
    "    #Initialize:\n",
    "    affprop_name = AffinityPropagation(affinity='precomputed', preference=pref, random_state=randstate)\n",
    "    affprop_name.fit(preproc_distances(dists, scalingfunc=defaultscaling)) #Scaling to increase separation\n",
    "    print('Clusters: ', len(np.unique(affprop_name.labels_)))\n",
    "    return affprop_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ac37a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit 1: Spelling\n",
      "Clusters:  220\n",
      "Fit 2: IPA\n",
      "Clusters:  211\n",
      "Fit 3: ARPABET\n",
      "Clusters:  175\n",
      "Fit 4: Triple-distance, sum\n",
      "Clusters:  217\n",
      "Fit 5: Triple-distance, vector\n",
      "Clusters:  207\n"
     ]
    }
   ],
   "source": [
    "#Simple versions: Use one distance each\n",
    "print('Fit 1: Spelling')\n",
    "aff_M_spelling = get_clusters(levdif_name_M, nametest_M_birth)\n",
    "print('Fit 2: IPA')\n",
    "aff_M_spelling = get_clusters(levdif_pronI_M, nametest_M_birth)\n",
    "print('Fit 3: ARPABET')\n",
    "aff_M_spelling = get_clusters(levdif_pronA_M, nametest_M_birth)\n",
    "\n",
    "#Aggregate version 1: Use sum for distance \n",
    "#(just add the lev distances together)\n",
    "print('Fit 4: Triple-distance, sum')\n",
    "levdif_temp = levdif_name_M + 0.75*levdif_pronI_M + 0.75*levdif_pronA_M\n",
    "aff_M_triple = get_clusters(levdif_temp, nametest_M_birth)\n",
    "\n",
    "#Aggregate version 2: Use vector length for distance \n",
    "#(treat each lev distance as independent axis, get vector length)\n",
    "print('Fit 5: Triple-distance, vector')\n",
    "levdif_temp = np.sqrt(    np.square(levdif_name_M) + \\\n",
    "                      0.5*np.square(levdif_pronI_M) + \\\n",
    "                      0.5*np.square(levdif_pronA_M))\n",
    "aff_M_triple2 = get_clusters(levdif_temp, nametest_M_birth)\n",
    "\n",
    "#Note: With these settings, random state doesn't matter, clusters are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a3b120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit 1: Spelling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chmullens/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_affinity_propagation.py:247: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "  \"will not have any cluster centers.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters:  1\n",
      "Fit 2: IPA\n",
      "Clusters:  517\n",
      "Fit 3: ARPABET\n",
      "Clusters:  393\n",
      "Fit 4: Triple-distance, sum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chmullens/opt/anaconda3/lib/python3.7/site-packages/sklearn/cluster/_affinity_propagation.py:247: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "  \"will not have any cluster centers.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters:  1\n",
      "Fit 5: Triple-distance, vector\n",
      "Clusters:  529\n"
     ]
    }
   ],
   "source": [
    "#Simple versions: Use one distance each\n",
    "print('Fit 1: Spelling')\n",
    "aff_S_spelling = get_clusters(levdif_name_S, nametest_S_birth)\n",
    "print('Fit 2: IPA')\n",
    "aff_S_spelling = get_clusters(levdif_pronI_S, nametest_S_birth)\n",
    "print('Fit 3: ARPABET')\n",
    "aff_S_spelling = get_clusters(levdif_pronA_S, nametest_S_birth)\n",
    "\n",
    "#Aggregate version 1: Use sum for distance \n",
    "#(just add the lev distances together)\n",
    "print('Fit 4: Triple-distance, sum')\n",
    "levdif_temp = levdif_name_S + 0.75*levdif_pronI_S + 0.75*levdif_pronA_S\n",
    "aff_S_triple = get_clusters(levdif_temp, nametest_S_birth)\n",
    "\n",
    "#Aggregate version 2: Use vector length for distance \n",
    "#(treat each lev distance as independent axis, get vector length)\n",
    "print('Fit 5: Triple-distance, vector')\n",
    "levdif_temp = np.sqrt(    np.square(levdif_name_S) + \\\n",
    "                      0.5*np.square(levdif_pronI_S) + \\\n",
    "                      0.5*np.square(levdif_pronA_S))\n",
    "aff_S_triple2 = get_clusters(levdif_temp, nametest_S_birth)\n",
    "\n",
    "#Note: With these settings, random state doesn't matter, clusters are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa2e1db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      " - *Aaron:* Aaron, Aron, Efren, Erin, Errol, Evan, Ezra\n",
      " - *Adriel:* Abram, Adriel, Efrain, Emil, Enrique, Ibrahim, Rodrigo, Unknown\n",
      " - *Alan:* Adam, Alan, Alden, Alec, Alex, Alfred, Allan, Allen, Alva, Arron, Atlas, Ivan\n",
      " - *Alejandro:* Alejandro, Alessandro, Leandro\n",
      " - *Alexander:* Alexander, Alexandro, Alexzander\n",
      " - *Alfonso:* Abdullah, Alberto, Alfonso, Alfredo, Alonso, Alonzo, Alphonse, Alphonso, Alvaro, Armando, Eduardo, Orlando, Osvaldo\n",
      " - *Ali:* Ali, Ari, Asa, Aubrey, Eli, Eloy, Oakley, Ollie, Otto\n",
      " - *Amari:* Aarav, Amari, Armani, Isai, Isaias, Kamari, Malachi, Malakai, Omari, Salvatore\n",
      " - *Andre:* Andre, Andreas, Andres, Andrew, Deandre\n",
      " - *Anthony:* Angelo, Anthony, Antony, Dangelo\n",
      " - *Antwan:* Antoine, Anton, Antwan, Antwon\n",
      " - *Arthur:* Archer, Archie, Arthur, Asher, Oliver, Oscar\n",
      " - *Austin:* Alton, Ashton, Austen, Austin, Dalton, Easton, Orval, Orville\n",
      " - *Ayden:* Abel, Adan, Aden, Adolph, Aidan, Aiden, Amos, Angel, Ayden, Eden, Enoch, Erwin, Ethan, Ian, Irwin, Isaac, Odin, Odis\n",
      " - *Bernardo:* Benito, Bernard, Bernardo, Domingo, Ernesto, Fernando, Gerard, Gerardo, Leonardo, Lorenzo, Raymundo, Reynaldo, Ricardo, Rodolfo, Rolando\n",
      " - *Blake:* Blaine, Blair, Blaise, Blake, Blaze, Clay\n",
      " - *Bradley:* Bradley, Bradly, Brantley, Stanley\n",
      " - *Brady:* Bailey, Brady, Brodie, Brody, Freddie, Freddy, Grady, Stacey, Stacy, Tracey, Tracy\n",
      " - *Brandon:* Bradford, Branden, Brandon, Branson, Braxton, Brendan, Brenden, Brendon, Brenton, Bronson, Draven, Francis, Franklin, Leonidas\n",
      " - *Brayden:* Braden, Braeden, Braiden, Brayden, Braydon, Braylen, Braylon, Brennan, Brennen, Clayton, Freeman, Grayson, Travon, Trevon\n",
      " - *Brent:* Brant, Brent, Bret, Brett, Brooks, Bruno, Clint, Kent, Prince, Trent\n",
      " - *Brian:* Brayan, Brian, Bryan, Bryant, Brycen, Bryon, Bryson, Darius, Graham, Orion\n",
      " - *Bryce:* Brad, Brice, Brock, Bruce, Bryce, Drew, Trace\n",
      " - *Callen:* Basil, Callen, Callum, Calvin, Cannon, Carol, Carroll, Cason, Collin, Cullen, Dallas, Darrius, Gannon, Jalen, Kaleb, Kason, Khalid, Khalil, Malik, Rashawn, Shannon, Talon, Wallace\n",
      " - *Cameron:* Cameron, Camren, Camron, Cassius, Kameron, Kamron, Maverick\n",
      " - *Carmelo:* Arturo, Carmelo, Deangelo, Gonzalo, Marcellus, Marcelo, Pasquale, Santino\n",
      " - *Carter:* Carter, Dante, Foster, Karter, Parker, Porter, Walter\n",
      " - *Cash:* Cash, Gus, Kash, Keith, Nash\n",
      " - *Chad:* Chad, Chaz, Chuck, Hal, Thad\n",
      " - *Charlie:* Barney, Charles, Charley, Charlie, Chauncey, Harley, Harvey, Jorge, Marty\n",
      " - *Chris:* Chris, Cliff, Cruz, Kareem, Kris, Quinn\n",
      " - *Christopher:* Christopher, Cristobal, Cristopher, Kristofer, Kristopher\n",
      " - *Claude:* Claud, Claude, Clyde\n",
      " - *Clinton:* Cleveland, Clifton, Clinton, Lincoln, Linwood, Quinten, Quintin, Quinton\n",
      " - *Cody:* Coby, Cody, Colby, Jody, Kobe, Kody, Kolby, Rudy, Toby, Tony\n",
      " - *Cole:* Cole, Colt, Kole, Kyle\n",
      " - *Colton:* Coleman, Colin, Colten, Colton, Holden, Kolton\n",
      " - *Connor:* Conner, Connor, Conor, Cooper, Gunnar, Gunner, Sawyer, Tanner\n",
      " - *Corey:* Barry, Buddy, Carey, Cary, Corey, Cory, Courtney, Kasey, Kerry, Korey, Kory, Kyree, Kyrie, Maurice, Rory, Sammie, Tory, Tyree, Tyrese\n",
      " - *Cornell:* Cordell, Cornell, Cortez, Darnell, Manuel, Marcel\n",
      " - *Dale:* Cale, Dale, Dane, Dave, Doyle, Gael, Gail, Gale, Lyle, Neal, Neil\n",
      " - *Darian:* Adrian, Adrien, Aryan, Damian, Damien, Damion, Danial, Darian, Darien, Dario, Darion, Daxton, Dorian, Fabian, Gabriel, Gideon, Marion, Simeon\n",
      " - *Darin:* Barrett, Channing, Daquan, Daren, Darin, Darren, Darrick, Darrin, Darwin, Davin, Donnell, Garret, Garrett, Gavin, Norris, Patrick, Travis\n",
      " - *Darrell:* Cyril, Daron, Darrel, Darrell, Darryl, Daryl, Dashawn, Deshawn, Devon, Merrill, Mitchell, Terrell, Virgil\n",
      " - *David:* David, Moises\n",
      " - *Dayton:* Burton, Curtis, Damon, Dana, Davion, Davis, Davon, Dawson, Dayton, Keaton, Kurtis, Layton, Leighton, Nathan, Payton, Peyton, Reagan, Tatum, Waylon\n",
      " - *Derek:* Denis, Dennis, Dereck, Derek, Derick, Derrick, Felix, Harris, Paris\n",
      " - *Dominic:* Demarcus, Domenic, Dominic, Dominick, Dominik, Dominique, Guadalupe, Lionel, Nathanael, Roosevelt, Salvador\n",
      " - *Don:* Bob, Dan, Don, Donte, Doug, Eldon, Juan, Lon, Ron\n",
      " - *Duane:* Drake, Duane, Dwain, Dwayne, Dwight\n",
      " - *Dylan:* Deacon, Declan, Devan, Deven, Dillan, Dillon, Douglas, Dylan, Kellan, Kellen, Kieran, Kylan, Mitchel, Philip, Phillip, Ryland, Willis, Zaiden, Zayden\n",
      " - *Earl:* Ace, Al, Earl, Earle, Merle\n",
      " - *Edison:* Abraham, Addison, Anderson, Atticus, Benjamin, Edison, Elisha, Emerson, Esteban, Estevan, Ezekiel, Garrison, Harrison, Hezekiah, Jamison, Jefferson, Remington\n",
      " - *Edmund:* Armand, Desmond, Edmond, Edmund, Edwin\n",
      " - *Elliott:* Ariel, Elian, Eliezer, Eliot, Elliot, Elliott, Everett, Everette, Isiah, Israel\n",
      " - *Elmer:* Delmar, Delmer, Edgar, Edward, Elmer, Elmo, Enzo, Ismael\n",
      " - *Elvin:* Alvin, Elton, Elvin, Elvis, Elwood, Ervin, Irvin, Irving, Melvin\n",
      " - *Emery:* Avery, Emery, Emory, Murray\n",
      " - *Emilio:* Adolfo, Antonio, Eliseo, Emilio, Gregorio, Ignacio, Isidro, Jedidiah, Mauricio, Maximilian, Octavio, Rogelio\n",
      " - *Emmanuel:* Demetrius, Emanuel, Emmanuel\n",
      " - *Eric:* Aric, Ellis, Emmett, Emmitt, Eric, Erich, Erick, Erik, Issac, Otis\n",
      " - *Ernest:* August, Earnest, Ernest\n",
      " - *Eugene:* Eugene, Joaquin, Johan, Uriel\n",
      " - *Francisco:* Francesco, Francisco, Giancarlo\n",
      " - *Frank:* Franco, Frank, Frankie, Grant, Hank\n",
      " - *Frederick:* Broderick, Cedric, Frederic, Frederick, Fredric, Fredrick, Hendrix, Kendrick, Roderick, Rodrick\n",
      " - *Gary:* Eddie, Eddy, Garry, Gary, Larry, Leroy, Mary, Perry, Teddy, Terry\n",
      " - *George:* George, Jarred, Jarrod\n",
      " - *Gerald:* Gerald, Harold, Jamari, Jared, Jarod, Jarrett, Jerald, Jeremiah, Jeremy, Jerold, Jerrod, Jerrold\n",
      " - *Gilberto:* Gilberto, Guillermo, Heriberto, Humberto, Rigoberto, Roberto, Sylvester, Vincenzo, Wilfredo\n",
      " - *Giovani:* Devante, Giovani, Giovanni, Giovanny, Jabari, Jacoby, Jovani\n",
      " - *Glenn:* Clair, Glen, Glenn, Greg, Gregg\n",
      " - *Herman:* German, Herman, Herschel, Hershel, Jermaine, Sherman, Thurman, Vernon\n",
      " - *Jace:* Chase, Gene, Hayes, Jace, Jair, Jake, Jase, Jay, Jayce, Jeff, Jess, Jett, Shane, Shayne\n",
      " - *Jackson:* Axel, Chadwick, Hamza, Jackson, Jadon, Jakob, Jamel, Jamil, Jaquan, Jasiah, Jaxon, Jaxson, Jaxton, Judson, Maxim, Maximo, Paxton, Samson\n",
      " - *James:* James\n",
      " - *Jason:* Hayden, Jacob, Jaden, Jaiden, Jameson, Jaron, Jason, Jayceon, Jayden, Jaydon, Jaylen, Jaylin, Jaylon, Jayson, Jesus, Mason, Seamus\n",
      " - *Jeffrey:* Geoffrey, Henry, Jeffery, Jeffrey, Jeffry, Shelby\n",
      " - *Jerry:* Gerry, Harry, Jairo, Jamie, Jerry, Jesse, Jessie, Jimmie, Jimmy, Shirley\n",
      " - *Joe:* Hugh, Jerome, Joe, Noe\n",
      " - *John:* Hans, Jack, Jacques, Jan, Jean, John, Jon, Josh\n",
      " - *Johnny:* Gianni, Jackie, Jagger, Jaime, Jamey, Joey, Johnie, Johnnie, Johnny, Scottie, Scotty\n",
      " - *Jonathan:* Donavan, Donovan, Jamarion, Johnathan, Johnathon, Jonathan, Jonathon, Solomon\n",
      " - *Jordan:* Corbin, Gordon, Harlan, Horace, Jarvis, Jordan, Jorden, Jordon, Korbin, Norman, Sheldon\n",
      " - *Jose:* Homer, Jose, Moshe\n",
      " - *Joseph:* Javon, Joel, Joesph, Jonah, Jonas, Josef, Joseph, Josue, Jovan, Judah, Yosef, Yusuf\n",
      " - *Julian:* Javion, Julian, Julien, Julio, Julius, Thaddeus, Ulises\n",
      " - *Justin:* Agustin, Augustus, Dustin, Houston, Hudson, Joshua, Justice, Justin, Justus, Shelton\n",
      " - *Kaden:* Caden, Caiden, Caleb, Cayden, Galen, Kaden, Kaiden, Kasen, Kayden, Kayson, Keagan, Keegan, Keenan, Keven, Kyson, Raiden\n",
      " - *Kai:* Beau, Bo, Coy, Guy, Kai, Lee, Ty\n",
      " - *Kane:* Cade, Cain, Case, Craig, Dean, Gage, Kade, Kale, Kane, Ken, Lane, Layne, Sage, Tate, Wayne, Zain, Zane, Zayn, Zayne\n",
      " - *Kenneth:* Beckett, Bennett, Kenneth, Kennith, Kermit, Phoenix\n",
      " - *Kenny:* Bennie, Benny, Bernie, Casey, Denny, Ernie, Kelly, Kenny, Kirby, Lenny, Percy, Quincy, Remy, Sonny\n",
      " - *Kenton:* Beckham, Benson, Benton, Denzel, Jensen, Kelvin, Kendall, Kenton, Kenyon, Kingston, Memphis, Nelson, Quentin, Vincent, Wendell, Weston, Winston\n",
      " - *Kevin:* Devin, Devyn, Kevin, Merlin, Mervin, Tevin\n",
      " - *Kim:* Bill, Dick, Finn, Jim, Kim, King, Kip, Lynn, Nick, Phil, Pierce, Rick, Tim, Vince, Will\n",
      " - *Kurt:* Bert, Curt, Dirk, Kirk, Kurt, Seth, Vern\n",
      " - *Lamar:* Hassan, Jamaal, Jamal, Jamar, Lamar, Lamont, Laverne, Milan, Rashad, Tomas\n",
      " - *Landon:* Anson, Camden, Conrad, Daniel, Duncan, Kamden, Landen, Landon, Landyn, London, Lyndon, Randal, Randall, Randell, Santos\n",
      " - *Lawrence:* Forest, Forrest, Laurence, Lawrence, Lawson, Leland, Loren, Maddox, Morris\n",
      " - *Leo:* Cleo, Leo, Levi, Theo\n",
      " - *Leon:* Deon, Keon, Kian, Leon, Liam\n",
      " - *Leonard:* Clifford, Lenard, Leonard, Maynard\n",
      " - *Lester:* Chester, Denver, Dexter, Hector, Hunter, Lester, Nestor, Victor, Xavier, Xzavier\n",
      " - *Lloyd:* Boyd, Floyd, Leif, Lloyd, Loyd, Wade\n",
      " - *Louis:* Dewey, Lewis, Louie, Louis, Luis\n",
      " - *Lucas:* Luca, Lucas, Lucian, Luka, Lukas, Rufus\n",
      " - *Luke:* Duke, Jude, Luke\n",
      " - *Mack:* Mac, Mack, Matt, Mike, Pat, Sam, Van, Zack\n",
      " - *Marco:* Aldo, Arlo, Carlo, Carlos, Marco, Marcos, Mario, Marquis, Matteo, Monroe, Pablo, Pedro, Rocco, Roscoe, Woodrow\n",
      " - *Mariano:* Emiliano, Federico, Gustavo, Keanu, Luciano, Mariano, Maximiliano, Santiago, Thiago\n",
      " - *Mark:* Bart, Carl, Clark, Garth, Karl, Marc, Mark, Marques, Myles, Ward\n",
      " - *Martin:* Arjun, Barton, Boston, Carlton, Carmen, Carmine, Carson, Garland, Karson, Malcolm, Marcus, Markus, Marlin, Marlon, Marquise, Marshall, Martin, Marvin, Matias, Morgan, Morton\n",
      " - *Matthew:* Mathew, Matthew, Samuel, Sergio\n",
      " - *Max:* Chance, Dax, Jax, Knox, Lance, Max, Miles, Ralph, Vance\n",
      " - *Messiah:* Dakota, Elias, Elijah, Isaiah, Izaiah, Josiah, Mateo, Mathias, Matthias, Messiah, Nehemiah, Tobias, Uriah, Yehuda\n",
      " - *Michael:* Cecil, Maxwell, Micah, Michael, Micheal, Mikel, Nigel, Russel, Russell, Silas\n",
      " - *Miguel:* Dewayne, Fidel, Jerrell, Makai, Michel, Miguel, Misael, Noel, Odell, Pierre\n",
      " - *Mohamed:* Mohamed, Mohammad, Mohammed, Muhammad\n",
      " - *Nathaniel:* Cornelius, Nathanial, Nathaniel, Sebastian\n",
      " - *Nicholas:* Finnegan, Maximus, Nicholas, Nickolas, Nicolas, Nikolai, Nikolas, Reginald, Sullivan, Theodore\n",
      " - *Nico:* Diego, Dino, Gino, Hugo, Milo, Nico, Niko, Rico, Vito\n",
      " - *Paul:* Paul, Raul, Saul\n",
      " - *Rafael:* Javier, Rafael, Raphael\n",
      " - *Randy:* Andy, Monte, Monty, Patsy, Randy, Sandy\n",
      " - *Ray:* Ray, Rey, Roy, Shea, Trey, Troy\n",
      " - *Reece:* Heath, Pete, Reece, Reese, Rhys, Royce\n",
      " - *Reid:* Ben, Bud, Ed, Fred, Jed, Ned, Reed, Reid, Rex, Rhett, Ted\n",
      " - *Richard:* Millard, Richard\n",
      " - *Ricky:* Billie, Billy, Booker, Mickey, Nicky, Reggie, Rickey, Rickie, Ricky, Timmy, Willie\n",
      " - *Robert:* Herbert, Lavern, Robert\n",
      " - *Roger:* Luther, Major, Rodger, Roger, Yahir\n",
      " - *Roman:* Logan, Moses, Nolan, Omar, Raymond, Reuben, Roland, Roman, Romeo, Ronan, Ronin, Ruben, Truman\n",
      " - *Ronald:* Arnold, Donald, Randolph, Rolland, Ronald, Rudolph\n",
      " - *Ronnie:* Bobbie, Bobby, Bodhi, Dannie, Danny, Donnie, Donny, Lanny, Lonnie, Rene, Riley, Robbie, Robby, Rocky, Rodney, Ronnie, Ronny, Sammy, Tommie, Tommy\n",
      " - *Rowan:* Bowen, Cohen, Lowell, Noah, Owen, Rohan, Rowan\n",
      " - *Ryan:* Chaim, Dion, Rayan, Robin, Royal, Ryan, Rylan, Wyatt, Zion\n",
      " - *Samir:* Akeem, Amir, Felipe, Jamir, Raheem, Ramiro, Ramon, Samir, Sincere, Tariq, Zaire\n",
      " - *Scott:* Scot, Scott, Semaj\n",
      " - *Sean:* Sean, Shaun, Shawn, Stan, Vaughn\n",
      " - *Stephan:* Clement, Griffin, Stefan, Stephan, Stephon, Sterling, Stetson\n",
      " - *Steven:* Seymour, Stephen, Steve, Steven, Stevie\n",
      " - *Stuart:* Buford, Howard, Hubert, Stewart, Stuart\n",
      " - *Terrence:* Clarence, Legend, Lennox, Terence, Terrance, Terrence, Theron\n",
      " - *Thomas:* Adonis, Ahmad, Ahmed, Hollis, Thomas\n",
      " - *Timothy:* Alijah, Deonte, Dimitri, Kennedy, Timothy\n",
      " - *Todd:* Rob, Rod, Ross, Tad, Tod, Todd, Tom\n",
      " - *Trevor:* Fletcher, Grover, River, Spencer, Trever, Trevor\n",
      " - *Triston:* Christian, Cristian, Greyson, Kristian, Preston, Princeton, Trenton, Tristan, Tristen, Tristian, Tristin, Triston\n",
      " - *Tyler:* Cesar, Iker, Kyler, Nasir, Peter, Ryder, Ryker, Skylar, Skyler, Taylor, Tucker, Turner, Tyler, Tylor, Walker, Wiley\n",
      " - *Tyron:* Byron, Cyrus, Hiram, Ira, Lyman, Myron, Simon, Soren, Titus, Tyrell, Tyron, Tyrone, Tyson, Warren\n",
      " - *Valentin:* Alexis, Augustine, Devonte, Ezequiel, Mackenzie, Ulysses, Valentin, Valentino, Vicente\n",
      " - *Wesley:* Ashley, Bentley, Dudley, Dusty, Finley, Leslie, Mekhi, Rusty, Sidney, Wesley, Westley\n",
      " - *Wilbert:* Albert, Delbert, Elbert, Gilbert, Milford, Norbert, Wilbert, Wilbur, Wilburn, Wilford, Willard, Wilmer\n",
      " - *William:* Killian, Leonel, Milton, Weldon, Wilfred, William, Wilson, Wilton, Winfred\n",
      " - *Xander:* Anders, Chandler, Dandre, Jasper, Junior, Sanford, Xander, Zander\n",
      " - *Zachary:* Gregory, Zachariah, Zachary, Zachery, Zackary, Zackery, Zechariah\n"
     ]
    }
   ],
   "source": [
    "def display_clusters(affprop, names, births):\n",
    "    print(len(np.unique(affprop.labels_)))\n",
    "    for cluster_id in np.unique(affprop.labels_):\n",
    "        exemplar = names[affprop.cluster_centers_indices_[cluster_id]]\n",
    "        cluster = np.unique(names[np.nonzero(affprop.labels_==cluster_id)])\n",
    "        cluster_str = \", \".join(cluster)\n",
    "        print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "\n",
    "display_clusters(aff_M_spelling, nametest_M_name, namebirth_M)\n",
    "\n",
    "    \n",
    "#Some of this ends up tracking which sounds are popular, which is fine.\n",
    "\n",
    "#The clusters are solid! Some names are a bit startling, but on\n",
    "#comparison it's clear why they were added. There isn't very much\n",
    "#difference between the vector and sum implementations of the three-\n",
    "#distance comparisons, so I'll stick with the vector version from\n",
    "#here on out. \n",
    "\n",
    "# tgt_affprop = affprop_M_triple\n",
    "# tgt_namebirth = namebirth_M\n",
    "\n",
    "# print(len(np.unique(tgt_affprop.labels_)))\n",
    "# for cluster_id in np.unique(tgt_affprop.labels_):\n",
    "#     exemplar = nametest_M_name[tgt_affprop.cluster_centers_indices_[cluster_id]]\n",
    "#     cluster = np.unique(nametest_M_name[np.nonzero(tgt_affprop.labels_==cluster_id)])\n",
    "#     cluster_str = \", \".join(cluster)\n",
    "#     print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5d4db",
   "metadata": {},
   "source": [
    "### Cluster verdict: Pretty good!\n",
    "\n",
    "The clusters above represent a sensible grouping of which names are most like one another, based on both spelling and estimated pronunciation. Some of the clusters are a little over-aggregated, but on the whole that's reasonable. I'm particularly glad that \"Esteban\" got included in the \"Steven\" group, which is a good sign. The clustering may be a little too aggressive, some clusters might be including elements that are not ideal. \n",
    "\n",
    "This can be tuned by modifying the scale of the preference or affinity structures to increase the relative impact of the precomputed distances. For now, I'll continue as-is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389af1bf",
   "metadata": {},
   "source": [
    "## Step 2: Modeling name popularity based on cluster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1abd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate cluster timecourses:\n",
    "\n",
    "cluster_courses = np.zeros([len(np.unique(tgt_affprop.labels_)),nametest_M_birth.shape[1]])\n",
    "for i,cluster_id in enumerate(np.unique(tgt_affprop.labels_)):\n",
    "    cc = nametest_M_birth[tgt_affprop.labels_==cluster_id,:]\n",
    "    cluster_courses[i,:] = np.sum(cc, axis=0)\n",
    "    \n",
    "#Normalize to year total -> fraction of year's births in X cluster\n",
    "cc_norm = cluster_courses/np.sum(cluster_courses, axis=0)\n",
    "#Normalize from fraction of year to standard scale\n",
    "cc_norm2 = (cc_norm - np.mean(cc_norm, axis=1).reshape(-1,1))/np.std(cc_norm, axis=1).reshape(-1,1)\n",
    "\n",
    "# plt.plot(cc_norm.T)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d65079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIT VERSION 1: SIMPLE LINEAR\n",
    "\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#PRELIMINARY SCORING:\n",
    "\n",
    "#Fit first-10 with visual verification:\n",
    "name_linmodels = []\n",
    "\n",
    "#The scores are often terrible, because it's comparing against\n",
    "#the mean of unknown data, but visually inspecting the first \n",
    "#30 or so it's actually doing a reasonable job of finding the\n",
    "#direction and shape of the trend in many cases. \n",
    "\n",
    "print('Fit all data before last 5 years, predict last 5:\\n')\n",
    "for n in range(30):\n",
    "    lin_mod = Ridge(alpha=0.01)\n",
    "\n",
    "    y_examp = nametest_M_birth[n,:].T\n",
    "    X_examp = np.hstack([cc_norm2.T, np.arange(1880,1880+n_years).reshape(-1,1)])\n",
    "\n",
    "    #Random split:\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_examp, y_examp, test_size=0.25)\n",
    "    #Year split:\n",
    "    X_train, X_test, y_train, y_test = X_examp[:-5], X_examp[-5:], y_examp[:-5], y_examp[-5:]\n",
    "\n",
    "    lin_mod.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = lin_mod.predict(X_test)\n",
    "    \n",
    "    print(nametest_M_name[n])\n",
    "    print(lin_mod.score(X_test, y_test))\n",
    "    \n",
    "    plt.plot(X_train[:,-1], y_train, 'b.')\n",
    "    plt.plot(X_test[:,-1], y_test, 'b+')\n",
    "    plt.plot(X_test[:,-1], y_pred, 'rx')\n",
    "    plt.legend(['Train','Test-actual','Test-pred'])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99333761",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#DETAILED SCORING WITH MISSING-YEAR TESTS:\n",
    "\n",
    "X_examp = np.hstack([cc_norm2.T, np.arange(1880,1880+n_years).reshape(-1,1)])\n",
    "\n",
    "#Re-fit using entire set prior to the last 5 years. Repeat this multiple\n",
    "#times using random sets of the data each time. \n",
    "\n",
    "n_refits = 20\n",
    "name_linmodels = []\n",
    "# name_linmodels = np.zeros([nametest_M_birth.shape[0], X_examp.shape[1], n_refits])\n",
    "name_linmodels_dic = {}\n",
    "test_scores = []\n",
    "\n",
    "for n in range(len(nametest_M_name)):\n",
    "    y_examp = nametest_M_birth[n,:].T\n",
    "\n",
    "    lin_mod = RidgeCV(cv=n_refits, alphas=[.1])\n",
    "\n",
    "    #Primitive bootstrap: Fit on partial data repeatedly, then average. \n",
    "    #Year split:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_examp[:-5], y_examp[:-5], test_size=0.1)\n",
    "    lin_mod.fit(X_train, y_train)\n",
    "\n",
    "    name_linmodels.append(lin_mod)\n",
    "    #name_linmodels[m, :, n] = lin_mod.coef_\n",
    "\n",
    "    test_score = lin_mod.score(X_examp[-5:], y_examp[-5:])\n",
    "    test_scores.append(test_score)\n",
    "    print('{0}: train {1:.3f}, test {2:.3f}, val {3:.3f} ({4})'.format(nametest_M_name[n],\n",
    "                                                                       lin_mod.score(X_train, y_train),\n",
    "                                                                       lin_mod.score(X_test, y_test),\n",
    "                                                                       test_score,\n",
    "                                                                       n\n",
    "                                                                      ))\n",
    "    y_pred_st = lin_mod.predict(X_test)\n",
    "    y_pred_fu = lin_mod.predict(X_examp[-5:])\n",
    "    if n%20==0:\n",
    "        plt.plot(X_train[:,-1], y_train, 'b.')\n",
    "        plt.plot(X_test[:,-1], y_test, 'b+')\n",
    "        plt.plot(X_test[:,-1], y_pred_st, 'rx')\n",
    "        plt.plot(X_examp[-5:,-1], y_examp[-5:], 'c+')\n",
    "        plt.plot(X_examp[-5:,-1], y_pred_fu[-5:], 'mx')\n",
    "        plt.legend(['Train','Test-actual','Test-pred','Val.-actual','Val.-pred'])\n",
    "        plt.show()\n",
    "    name_linmodels_dic[nametest_M_name[n]] = lin_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb8f40",
   "metadata": {},
   "source": [
    "As you can see in the second set of test plots, while the scores are very poor, that's because the scoring compares them against the *known* mean value of the last five years. They're actually reasonable predictions given the past history of the name in most cases! Even hitting a score of zero (performing as well as the actual mean of the last five years) is quite good. There are a few notable problems, though. For example, the estimates are often overly-optimistic for names that are currently in a long-term low. It really thought Bert and Cletus were going to come back, for example. Similarly, it sometimes heads significantly negative, which is an obvious problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(name_linmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9bd7a",
   "metadata": {},
   "source": [
    "## Name trend detection preliminary conclusion:\n",
    "\n",
    "With variable preprocessing, a simple linear model does a... decent job of tracking name trends without drastic overfitting. I'd need to extend this significantly to allow next-year prediction (i.e. generating projected name-cluster data), but it's a clear sign that we can reasonably track some trends for relatively popular names. To use the prior model for predictions, we'd have to generate synthetic cluster-trend data, which is feasible. For example, let's plug what we have into a time-series model like ARIMA, which uses the time course of the data to project its future trajectory. This is like to do reasonably well for near-future projection, and we can use that projection to generate the synthetic cluster trends necessary for the linear model to predict. Of course, we could also use the ARIMA directly on each name, but that would take hours for the full set of names compared to seconds for the linear model, and would also discard the information we can calculate about how names trend together. \n",
    "\n",
    "*(This might actually be a pretty good spot for a multidimensional Kalman filter, which would use information from each timecourse variable quite nicely. I've been waiting for an excuse to get one of those going, might finally be the time!)*  \n",
    "*(There are 0 babies named Arima in the names database, which is honestly a little surprising.)*\n",
    "\n",
    "## ARIMA name timecourse projection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9481a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names_df_trim[names_df_trim['Name'] == 'Arima'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e23580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "\n",
    "#Handy reference for transforms:\n",
    "#https://alkaline-ml.com/pmdarima/usecases/sun-spots.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Testing this on most recent 5 years, so all but last 5 in training set\n",
    "tsize = 135\n",
    "\n",
    "num_clust = cc_norm2.shape[0]\n",
    "\n",
    "arima_mods = []\n",
    "for n in range(num_clust):\n",
    "    y = cc_norm2[n,:]\n",
    "    y_train, y_test = train_test_split(y, train_size=tsize)\n",
    "\n",
    "    arima_mod = pm.auto_arima(y_train)\n",
    "\n",
    "    y_pred = arima_mod.predict(y_test.shape[0])  # predict N steps into the future\n",
    "\n",
    "    ## Visualize the forecasts (blue=train, green=forecasts)\n",
    "    #x = np.arange(y.shape[0])\n",
    "    #plt.plot(x[:tsize], y_train, c='blue')\n",
    "    #plt.plot(x[tsize:], y[tsize:], 'b--')\n",
    "    #plt.plot(x[tsize:], y_pred, c='green')\n",
    "    #plt.show()\n",
    "    \n",
    "    arima_mods.append(arima_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the predicted-cluster-trend matrix\n",
    "\n",
    "y_pred_arima = np.zeros([num_clust, 5])\n",
    "for n in range(num_clust):\n",
    "    y_pred_arima[n,:] = arima_mods[n].predict(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced45251",
   "metadata": {},
   "source": [
    "## Actual name trend predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#STEP 1: REDO THE ARIMAS WITH ALL YEARS\n",
    "\n",
    "tsize = 140\n",
    "\n",
    "num_clust = cc_norm2.shape[0]\n",
    "\n",
    "arima_mods = []\n",
    "for n in range(num_clust):\n",
    "    y = cc_norm2[n,:]\n",
    "    #y_train, y_test = train_test_split(y, train_size=tsize)\n",
    "    y_train = y\n",
    "\n",
    "    arima_mod = pm.auto_arima(y_train)\n",
    "\n",
    "    y_pred = arima_mod.predict(5)  # predict N steps into the future\n",
    "\n",
    "    ## Visualize the forecasts (blue=train, green=forecasts)\n",
    "    #x = np.arange(y.shape[0])\n",
    "    #plt.plot(x[:tsize], y_train, c='blue')\n",
    "    #plt.plot(x[tsize:], y[tsize:], 'b--')\n",
    "    #plt.plot(x[tsize:], y_pred, c='green')\n",
    "    #plt.show()\n",
    "    \n",
    "    arima_mods.append(arima_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74533ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_arima.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the predicted-cluster-trend matrix\n",
    "\n",
    "y_pred_arima = np.zeros([num_clust, 5])\n",
    "for n in range(num_clust):\n",
    "    y_pred_arima[n,:] = arima_mods[n].predict(5)\n",
    "\n",
    "#Add the pre-predicted value to the stack to convert the 5\n",
    "#predicted values to a change (6 vals -> 5 differences)\n",
    "baseval = cc_norm2[:,-1]\n",
    "y_pred_arima = np.hstack([baseval.reshape(-1,1),y_pred_arima])\n",
    "#Calculate the X-predicted in the difference format:\n",
    "X_pred_arima = np.hstack([np.diff(y_pred_arima).T, np.arange(2020,2025).reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03aaad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_arima.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121764bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_examp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#STEP 2: Redo the linear fits to include the complete dataset.\n",
    "\n",
    "#1. Convert to CHANGE by year (solves most of the negatives problems)\n",
    "temp_delta = np.diff(cc_norm2) #yes, correct axis (1) is default\n",
    "temp_bdelta = np.diff(nametest_M_birth) #^same\n",
    "\n",
    "#2. Start in 1881, since that's the first year we can do diff for\n",
    "X_examp = np.hstack([temp_delta.T, np.arange(1881,2020).reshape(-1,1)])\n",
    "\n",
    "n_refits = 20\n",
    "name_linmodels = []\n",
    "name_linmodels_dic = {}\n",
    "test_scores = []\n",
    "\n",
    "for n in range(len(nametest_M_name)):\n",
    "    y_examp = temp_bdelta[n,:].T\n",
    "    \n",
    "    lin_mod = RidgeCV(cv=n_refits, alphas=[.1])\n",
    "    \n",
    "    #With this few data points, I'm throwing the whole thing in for now.\n",
    "    #The CV is using a subset each time already.\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_examp, y_examp, test_size=0.1)\n",
    "    X_train, y_train = X_examp, y_examp\n",
    "    lin_mod.fit(X_train, y_train)\n",
    "\n",
    "    name_linmodels.append(lin_mod)\n",
    "    \n",
    "    print('{0}: train {1:.3f} ({2})'.format(nametest_M_name.iloc[n],lin_mod.score(X_train, y_train), n))\n",
    "    y_pred_fu = lin_mod.predict(X_pred_arima)\n",
    "    if n%20==0:\n",
    "        plt.plot(X_train[:,-1], y_train, 'b.')\n",
    "        #plt.plot(X_test[:,-1], y_test, 'b+')\n",
    "        #plt.plot(X_test[:,-1], y_pred_st, 'rx')\n",
    "        #plt.plot(X_examp[-5:,-1], y_examp[-5:], 'c+')\n",
    "        #plt.plot(X_examp[-5:,-1], y_pred_fu[-5:], 'mx')\n",
    "        plt.plot(np.arange(2020,2025), y_pred_fu)\n",
    "        plt.legend(['Train','Test-actual','Test-pred','Val.-actual','Val.-pred'])\n",
    "        plt.show()\n",
    "    name_linmodels_dic[nametest_M_name.iloc[n]] = lin_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3: MAKE NAME PROJECTIONS\n",
    "\n",
    "n_names = len(nametest_M_name)\n",
    "\n",
    "name_futures = np.zeros([n_names,5])\n",
    "for n in range(n_names):\n",
    "    name_futures[n,:] = name_linmodels[n].predict(X_pred_arima)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testrun: First 30 names\n",
    "\n",
    "for n in range(30):\n",
    "    plt.plot(np.arange(1880,2020), nametest_M_birth[n,:])\n",
    "    plt.plot(np.arange(2020,2025), nametest_M_birth[n,-1] + name_futures[n,:] - name_futures[n,0], 'b+')\n",
    "    plt.title(nametest_M_name.iloc[n])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e39dba6",
   "metadata": {},
   "source": [
    "Modifications: I should do a log transform on the data early on, to convert it closer to a normal-distribution range of values, which would respond more appropriately to linear fitting. That's going near the top of the list, it will have a real effect on how well most models can characterize the data. \n",
    "\n",
    "Overly-optimistic predictions for near-dead names could be handled better at the individual-name level with ARIMA, despite the drastically slower processing. Similarly, ARIMA would likely also do fairly well avoiding negatives, since the graph tends to flatten as it nears zero. That's largely handled with the transition to yearly change, though. A hybrid model using both could probably do pretty well, but it would be difficult to reasonably fit given the data, and at that point you might want to head into more complex territory like RNN. \n",
    "\n",
    "Anyway, predicting names is inherently a difficult problem. We don't have reliable time components to the signal that we can account for, or a massive pile of datapoints. There are generation-related timing elements, but those are close to random, and how long trends persist may be changing. \n",
    "\n",
    "Pulling in data on name popularity on a state-by-state basis would help quite a bit, bumping our dataset from about 140 datapoints to about 5000 (some state reporting didn't get going until later if I'm recalling right, so it's not quite 140 x 51). It certainly wouldn't solve the problem by itself! State name trends are strongly correlated with national trends, especially as media increasingly moves away from the local level. State data could significantly help, but it will be necessary to use models that account for correlation between variables. \n",
    "\n",
    "Future directions for name trend prediction, in summary:\n",
    "- More in-depth data transformation prior to analysis/modeling (log or Box-Cox transform built into `pmdarima`, for example)\n",
    "- Consider multivariate autoregression (i.e. VAR) to model multivariate time series data more effectively (not likely to make the ARIMA fitting feasible for individual names without a days-long run)  \n",
    "- Consider reweighting recent years. Names with a strong recent increase are at a disadvantage in threshold, despite strong likelihood of continuing future presence. Could weigh name prevalence more highly in the last 3-5 years, for example, both to avoid cutting them off at the threshold stage and to increase the attention the model pays to recent trends? Something to consider.\n",
    "- Add state data to improve models (at minimum should reduce year-level noise, important now that it's predicting based on yearly change)\n",
    "- Toss this into an actual user interface/prediction engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assorted fun side comparisons:\n",
    "\n",
    "#Names in '50 ways to leave your lover' by Paul Simon:\n",
    "#https://www.azlyrics.com/lyrics/paulsimon/50waystoleaveyourlover.html\n",
    "#Jack, Stan, Roy, Gus, Lee\n",
    "\n",
    "#Names in 'Mambo No. 5' by Lou Bega:\n",
    "#https://www.azlyrics.com/lyrics/loubega/mambono5.html\n",
    "#Angela, Pamela, Sandra, Rita, Monica, Erica, Rita (again), Tina, Sandra, Mary, Jessica\n",
    "\n",
    "#Names in '88 lines about 44 wome' by The Nails:\n",
    "#https://genius.com/The-nails-88-lines-about-44-women-lyrics\n",
    "#Deborah, Carla, Mary, Susan, Reno, Cathy, Vicki, Kamela, Xylla, Joan, Sherry, Kathleen,\n",
    "#Seattle, Karen, Jeannie, Mary Ellen, Gloria, Mimi, Marilyn, Julie, Rhonda, Patty, Linda, \n",
    "#Katherine, Pauline, Jean-Marie, Gina, Jackie, Sarah, Janet, Tanya, Brenda, Rowena, Dee Dee, \n",
    "#Debbie Ray, Nina, Bobbi, Eloise, Terri, Ronnie, Jezebel, Dinah, Judy, Amaranta\n",
    "\n",
    "#Country/state names:\n",
    "#(self-evident)\n",
    "\n",
    "#City names: \n",
    "#US cities by population (>100,000):\n",
    "#https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\n",
    "\n",
    "#Worldwide cities (>5,000,000):\n",
    "#https://en.wikipedia.org/wiki/List_of_largest_cities\n",
    "\n",
    "#Urban vs. rural states:\n",
    "#https://fivethirtyeight.com/features/how-urban-or-rural-is-your-state-and-what-does-that-mean-for-the-2020-election/\n",
    "#(Loosely, how many people are within 5 miles of a given census tract)\n",
    "\n",
    "#Check trendiness: It looks in general like the time course for F\n",
    "#names may be shorter than the time course for M names. Is that\n",
    "#true in the past? Is it continuing to be true for both sexes?\n",
    "#How:\n",
    "#Could look at that by taking 10-year chunks (looks like about the\n",
    "#right length to capture a rising or falling trend) and seeing which\n",
    "#names correlate together for each chunk? Automatic grouping based\n",
    "#on \"trendiness\" would be pretty cool. \n",
    "\n",
    "#Extend data: I could also cross-correlate names with Google ngrams \n",
    "#results for that name, that would be a nice comparison. Excellent\n",
    "#proxy for whether a name is culturally embedded in the US!\n",
    "\n",
    "#Extend data: Cross-correlate artists who made the Billboard top 10\n",
    "#using their names (i.e. Alanis). Only include cases where the name\n",
    "#is in both datasets, so a simple inner join would be ideal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff141d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
